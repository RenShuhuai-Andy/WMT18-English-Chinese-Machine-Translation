2021-03-05 11:59:05 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15939
2021-03-05 11:59:05 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15939
2021-03-05 11:59:05 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:15939
2021-03-05 11:59:06 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15939
2021-03-05 11:59:06 | INFO | fairseq.distributed_utils | initialized host lanco16 as rank 1
2021-03-05 11:59:06 | INFO | fairseq.distributed_utils | initialized host lanco16 as rank 2
2021-03-05 11:59:06 | INFO | fairseq.distributed_utils | initialized host lanco16 as rank 3
2021-03-05 11:59:06 | INFO | fairseq.distributed_utils | initialized host lanco16 as rank 0
2021-03-05 11:59:11 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_wmt_en_zh_big', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data/data-bin-joint', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:15939', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=True, eval_bleu_args='{"beam": 4, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=40, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, local_rank=0, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=25000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, print_attn_score=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer_wmt_en_zh_big', save_interval=1, save_interval_updates=2000, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='model', valid_subset='valid', validate_after_updates=0, validate_interval=3, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')
2021-03-05 11:59:11 | INFO | fairseq.tasks.translation | [en] dictionary: 32768 types
2021-03-05 11:59:11 | INFO | fairseq.tasks.translation | [zh] dictionary: 32768 types
2021-03-05 11:59:11 | INFO | fairseq.data.data_utils | loaded 23809 examples from: data/data-bin-joint/valid.en-zh.en
2021-03-05 11:59:11 | INFO | fairseq.data.data_utils | loaded 23809 examples from: data/data-bin-joint/valid.en-zh.zh
2021-03-05 11:59:11 | INFO | fairseq.tasks.translation | data/data-bin-joint valid en-zh 23809 examples
2021-03-05 11:59:12 | INFO | fairseq_cli.train | OurTransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(32768, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): OurTransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(32768, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=32768, bias=False)
  )
)
2021-03-05 11:59:12 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-03-05 11:59:12 | INFO | fairseq_cli.train | model: transformer_wmt_en_zh_big (OurTransformerModel)
2021-03-05 11:59:12 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2021-03-05 11:59:12 | INFO | fairseq_cli.train | num. model params: 60915712 (num. trained: 60915712)
2021-03-05 11:59:13 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-03-05 11:59:13 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-03-05 11:59:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2021-03-05 11:59:13 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2021-03-05 11:59:13 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2021-03-05 11:59:13 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2021-03-05 11:59:13 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2021-03-05 11:59:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2021-03-05 11:59:13 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2021-03-05 11:59:13 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2021-03-05 11:59:13 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/transformer_wmt_en_zh_big/checkpoint_last.pt
2021-03-05 11:59:13 | INFO | fairseq.trainer | loading train data for epoch 1
2021-03-05 11:59:13 | INFO | fairseq.data.data_utils | loaded 166685 examples from: data/data-bin-joint/train.en-zh.en
2021-03-05 11:59:13 | INFO | fairseq.data.data_utils | loaded 166685 examples from: data/data-bin-joint/train.en-zh.zh
2021-03-05 11:59:13 | INFO | fairseq.tasks.translation | data/data-bin-joint train en-zh 166685 examples
2021-03-05 11:59:13 | INFO | fairseq.trainer | begin training epoch 1
2021-03-05 11:59:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2021-03-05 11:59:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2021-03-05 12:00:03 | INFO | train_inner | epoch 001:    102 / 308 loss=14.091, nll_loss=13.902, ppl=15313.2, wps=30239.2, ups=2.15, wpb=14100.9, bsz=528.7, num_updates=100, lr=1.25e-05, gnorm=2.93, loss_scale=32, train_wall=47, wall=50
2021-03-05 12:00:49 | INFO | train_inner | epoch 001:    202 / 308 loss=12.54, nll_loss=12.174, ppl=4621.58, wps=30670.8, ups=2.15, wpb=14248.9, bsz=570.3, num_updates=200, lr=2.5e-05, gnorm=1.389, loss_scale=32, train_wall=46, wall=96
2021-03-05 12:01:37 | INFO | train_inner | epoch 001:    302 / 308 loss=11.416, nll_loss=10.89, ppl=1898, wps=29573.1, ups=2.11, wpb=13990.1, bsz=526.9, num_updates=300, lr=3.75e-05, gnorm=1.39, loss_scale=32, train_wall=47, wall=144
2021-03-05 12:01:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:01:42 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint1.pt (epoch 1 @ 306 updates, score None) (writing took 2.765300778206438 seconds)
2021-03-05 12:01:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-03-05 12:01:43 | INFO | train | epoch 001 | loss 12.653 | nll_loss 12.288 | ppl 5000.95 | wps 29569.8 | ups 2.09 | wpb 14118.5 | bsz 540.8 | num_updates 306 | lr 3.825e-05 | gnorm 1.887 | loss_scale 32 | train_wall 144 | wall 149
2021-03-05 12:01:43 | INFO | fairseq.trainer | begin training epoch 2
2021-03-05 12:02:28 | INFO | train_inner | epoch 002:     94 / 308 loss=10.855, nll_loss=10.2, ppl=1176.09, wps=27794.6, ups=1.96, wpb=14213.4, bsz=546.9, num_updates=400, lr=5e-05, gnorm=1.342, loss_scale=32, train_wall=47, wall=195
2021-03-05 12:03:15 | INFO | train_inner | epoch 002:    194 / 308 loss=10.696, nll_loss=9.984, ppl=1012.55, wps=30292.5, ups=2.15, wpb=14111.6, bsz=536.1, num_updates=500, lr=6.25e-05, gnorm=1.428, loss_scale=32, train_wall=46, wall=241
2021-03-05 12:04:01 | INFO | train_inner | epoch 002:    294 / 308 loss=10.522, nll_loss=9.781, ppl=879.86, wps=30163, ups=2.14, wpb=14110.1, bsz=546.4, num_updates=600, lr=7.5e-05, gnorm=1.211, loss_scale=32, train_wall=47, wall=288
2021-03-05 12:04:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:04:14 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint2.pt (epoch 2 @ 614 updates, score None) (writing took 6.499384362716228 seconds)
2021-03-05 12:04:14 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-03-05 12:04:14 | INFO | train | epoch 002 | loss 10.676 | nll_loss 9.969 | ppl 1002.55 | wps 28622.1 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 614 | lr 7.675e-05 | gnorm 1.336 | loss_scale 32 | train_wall 144 | wall 301
2021-03-05 12:04:15 | INFO | fairseq.trainer | begin training epoch 3
2021-03-05 12:04:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2021-03-05 12:04:55 | INFO | train_inner | epoch 003:     87 / 308 loss=10.34, nll_loss=9.573, ppl=761.88, wps=25934.2, ups=1.85, wpb=14030.2, bsz=532, num_updates=700, lr=8.75e-05, gnorm=1.222, loss_scale=16, train_wall=46, wall=342
2021-03-05 12:05:43 | INFO | train_inner | epoch 003:    187 / 308 loss=10.174, nll_loss=9.384, ppl=667.96, wps=29557.1, ups=2.09, wpb=14136.4, bsz=531, num_updates=800, lr=0.0001, gnorm=1.229, loss_scale=16, train_wall=48, wall=390
2021-03-05 12:06:30 | INFO | train_inner | epoch 003:    287 / 308 loss=9.947, nll_loss=9.124, ppl=558.13, wps=30225.8, ups=2.13, wpb=14166.8, bsz=550.7, num_updates=900, lr=0.0001125, gnorm=1.346, loss_scale=16, train_wall=47, wall=437
2021-03-05 12:06:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 12:12:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.68 | nll_loss 8.792 | ppl 443.12 | bleu 0.14 | wps 1938.9 | wpb 12732.6 | bsz 485.9 | num_updates 921
2021-03-05 12:12:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:12:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint3.pt (epoch 3 @ 921 updates, score 0.14) (writing took 7.22872129175812 seconds)
2021-03-05 12:12:19 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-03-05 12:12:19 | INFO | train | epoch 003 | loss 10.118 | nll_loss 9.319 | ppl 638.78 | wps 8949.9 | ups 0.63 | wpb 14122.1 | bsz 540.4 | num_updates 921 | lr 0.000115125 | gnorm 1.277 | loss_scale 16 | train_wall 143 | wall 786
2021-03-05 12:12:19 | INFO | fairseq.trainer | begin training epoch 4
2021-03-05 12:12:57 | INFO | train_inner | epoch 004:     79 / 308 loss=9.743, nll_loss=8.891, ppl=474.65, wps=3614.9, ups=0.26, wpb=13968.4, bsz=532, num_updates=1000, lr=0.000125, gnorm=1.317, loss_scale=16, train_wall=46, wall=824
2021-03-05 12:13:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2021-03-05 12:13:43 | INFO | train_inner | epoch 004:    180 / 308 loss=9.561, nll_loss=8.682, ppl=410.68, wps=30217.8, ups=2.14, wpb=14090.5, bsz=518.4, num_updates=1100, lr=0.0001375, gnorm=1.279, loss_scale=8, train_wall=46, wall=870
2021-03-05 12:14:30 | INFO | train_inner | epoch 004:    280 / 308 loss=9.367, nll_loss=8.457, ppl=351.42, wps=30844.5, ups=2.16, wpb=14295.7, bsz=559.9, num_updates=1200, lr=0.00015, gnorm=1.288, loss_scale=8, train_wall=46, wall=917
2021-03-05 12:14:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:14:50 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint4.pt (epoch 4 @ 1228 updates, score None) (writing took 6.856427097693086 seconds)
2021-03-05 12:14:50 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-03-05 12:14:50 | INFO | train | epoch 004 | loss 9.505 | nll_loss 8.617 | ppl 392.53 | wps 28780.7 | ups 2.04 | wpb 14122.6 | bsz 539.7 | num_updates 1228 | lr 0.0001535 | gnorm 1.27 | loss_scale 8 | train_wall 142 | wall 936
2021-03-05 12:14:50 | INFO | fairseq.trainer | begin training epoch 5
2021-03-05 12:15:24 | INFO | train_inner | epoch 005:     72 / 308 loss=9.172, nll_loss=8.235, ppl=301.32, wps=25682.3, ups=1.83, wpb=14040.4, bsz=545.7, num_updates=1300, lr=0.0001625, gnorm=1.238, loss_scale=8, train_wall=46, wall=971
2021-03-05 12:16:11 | INFO | train_inner | epoch 005:    172 / 308 loss=9.018, nll_loss=8.056, ppl=266.15, wps=29798.1, ups=2.12, wpb=14059, bsz=536.7, num_updates=1400, lr=0.000175, gnorm=1.326, loss_scale=8, train_wall=47, wall=1018
2021-03-05 12:16:58 | INFO | train_inner | epoch 005:    272 / 308 loss=8.803, nll_loss=7.81, ppl=224.39, wps=30834.7, ups=2.15, wpb=14342.2, bsz=556, num_updates=1500, lr=0.0001875, gnorm=1.181, loss_scale=8, train_wall=46, wall=1065
2021-03-05 12:17:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:17:21 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint5.pt (epoch 5 @ 1536 updates, score None) (writing took 6.562799833714962 seconds)
2021-03-05 12:17:21 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-03-05 12:17:21 | INFO | train | epoch 005 | loss 8.946 | nll_loss 7.974 | ppl 251.44 | wps 28644.3 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 1536 | lr 0.000192 | gnorm 1.258 | loss_scale 8 | train_wall 143 | wall 1088
2021-03-05 12:17:22 | INFO | fairseq.trainer | begin training epoch 6
2021-03-05 12:17:53 | INFO | train_inner | epoch 006:     64 / 308 loss=8.628, nll_loss=7.609, ppl=195.16, wps=25563.7, ups=1.83, wpb=13938.4, bsz=528, num_updates=1600, lr=0.0002, gnorm=1.213, loss_scale=8, train_wall=46, wall=1119
2021-03-05 12:18:39 | INFO | train_inner | epoch 006:    164 / 308 loss=8.463, nll_loss=7.417, ppl=170.94, wps=30539.9, ups=2.17, wpb=14089.9, bsz=538.3, num_updates=1700, lr=0.0002125, gnorm=1.182, loss_scale=8, train_wall=46, wall=1166
2021-03-05 12:19:26 | INFO | train_inner | epoch 006:    264 / 308 loss=8.301, nll_loss=7.229, ppl=150.01, wps=30090.7, ups=2.11, wpb=14238.6, bsz=551.4, num_updates=1800, lr=0.000225, gnorm=1.231, loss_scale=8, train_wall=47, wall=1213
2021-03-05 12:19:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 12:25:19 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.923 | nll_loss 6.742 | ppl 107.06 | bleu 1.3 | wps 1938 | wpb 12732.6 | bsz 485.9 | num_updates 1844 | best_bleu 1.3
2021-03-05 12:25:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:25:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint6.pt (epoch 6 @ 1844 updates, score 1.3) (writing took 11.282829549629241 seconds)
2021-03-05 12:25:30 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-03-05 12:25:30 | INFO | train | epoch 006 | loss 8.393 | nll_loss 7.336 | ppl 161.61 | wps 8901.4 | ups 0.63 | wpb 14123.4 | bsz 541.2 | num_updates 1844 | lr 0.0002305 | gnorm 1.181 | loss_scale 8 | train_wall 143 | wall 1577
2021-03-05 12:25:30 | INFO | fairseq.trainer | begin training epoch 7
2021-03-05 12:25:58 | INFO | train_inner | epoch 007:     56 / 308 loss=8.109, nll_loss=7.009, ppl=128.79, wps=3629.2, ups=0.26, wpb=14219.1, bsz=551, num_updates=1900, lr=0.0002375, gnorm=1.168, loss_scale=8, train_wall=47, wall=1605
2021-03-05 12:26:45 | INFO | train_inner | epoch 007:    156 / 308 loss=7.962, nll_loss=6.837, ppl=114.35, wps=29726.6, ups=2.11, wpb=14095.8, bsz=533.4, num_updates=2000, lr=0.00025, gnorm=1.106, loss_scale=8, train_wall=47, wall=1652
2021-03-05 12:26:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 12:32:07 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.705 | nll_loss 6.486 | ppl 89.65 | bleu 1.86 | wps 1999.5 | wpb 12732.6 | bsz 485.9 | num_updates 2000 | best_bleu 1.86
2021-03-05 12:32:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:32:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_7_2000.pt (epoch 7 @ 2000 updates, score 1.86) (writing took 11.027642370201647 seconds)
2021-03-05 12:33:06 | INFO | train_inner | epoch 007:    256 / 308 loss=7.87, nll_loss=6.729, ppl=106.06, wps=3695.1, ups=0.26, wpb=14053.1, bsz=533.6, num_updates=2100, lr=0.0002625, gnorm=1.114, loss_scale=8, train_wall=47, wall=2032
2021-03-05 12:33:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:33:37 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint7.pt (epoch 7 @ 2152 updates, score None) (writing took 6.438193153124303 seconds)
2021-03-05 12:33:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-03-05 12:33:37 | INFO | train | epoch 007 | loss 7.909 | nll_loss 6.776 | ppl 109.57 | wps 8935.3 | ups 0.63 | wpb 14123.4 | bsz 541.2 | num_updates 2152 | lr 0.000269 | gnorm 1.135 | loss_scale 8 | train_wall 145 | wall 2064
2021-03-05 12:33:37 | INFO | fairseq.trainer | begin training epoch 8
2021-03-05 12:34:00 | INFO | train_inner | epoch 008:     48 / 308 loss=7.667, nll_loss=6.496, ppl=90.29, wps=25825.5, ups=1.83, wpb=14146.5, bsz=549.5, num_updates=2200, lr=0.000275, gnorm=1.147, loss_scale=8, train_wall=47, wall=2087
2021-03-05 12:34:48 | INFO | train_inner | epoch 008:    148 / 308 loss=7.5, nll_loss=6.302, ppl=78.93, wps=29655.7, ups=2.1, wpb=14137.4, bsz=553.1, num_updates=2300, lr=0.0002875, gnorm=1.073, loss_scale=8, train_wall=47, wall=2135
2021-03-05 12:35:34 | INFO | train_inner | epoch 008:    248 / 308 loss=7.424, nll_loss=6.212, ppl=74.13, wps=30520.8, ups=2.16, wpb=14152.8, bsz=540.5, num_updates=2400, lr=0.0003, gnorm=1.119, loss_scale=8, train_wall=46, wall=2181
2021-03-05 12:36:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:36:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint8.pt (epoch 8 @ 2460 updates, score None) (writing took 6.48910213727504 seconds)
2021-03-05 12:36:09 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-03-05 12:36:09 | INFO | train | epoch 008 | loss 7.467 | nll_loss 6.263 | ppl 76.81 | wps 28687.6 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 2460 | lr 0.0003075 | gnorm 1.108 | loss_scale 8 | train_wall 143 | wall 2216
2021-03-05 12:36:09 | INFO | fairseq.trainer | begin training epoch 9
2021-03-05 12:36:29 | INFO | train_inner | epoch 009:     40 / 308 loss=7.292, nll_loss=6.059, ppl=66.65, wps=25920.2, ups=1.84, wpb=14119.5, bsz=528.6, num_updates=2500, lr=0.0003125, gnorm=1.096, loss_scale=8, train_wall=47, wall=2236
2021-03-05 12:37:16 | INFO | train_inner | epoch 009:    140 / 308 loss=7.069, nll_loss=5.802, ppl=55.8, wps=30189.5, ups=2.14, wpb=14121.1, bsz=543.1, num_updates=2600, lr=0.000325, gnorm=1.04, loss_scale=8, train_wall=47, wall=2283
2021-03-05 12:38:03 | INFO | train_inner | epoch 009:    240 / 308 loss=6.958, nll_loss=5.672, ppl=50.99, wps=30007.4, ups=2.13, wpb=14064.9, bsz=533.4, num_updates=2700, lr=0.0003375, gnorm=1.02, loss_scale=8, train_wall=47, wall=2329
2021-03-05 12:38:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 12:44:02 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.706 | nll_loss 5.27 | ppl 38.58 | bleu 3.71 | wps 1968.5 | wpb 12732.6 | bsz 485.9 | num_updates 2768 | best_bleu 3.71
2021-03-05 12:44:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:44:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint9.pt (epoch 9 @ 2768 updates, score 3.71) (writing took 11.002944960258901 seconds)
2021-03-05 12:44:13 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-03-05 12:44:13 | INFO | train | epoch 009 | loss 7.003 | nll_loss 5.725 | ppl 52.89 | wps 8988.9 | ups 0.64 | wpb 14123.4 | bsz 541.2 | num_updates 2768 | lr 0.000346 | gnorm 1.031 | loss_scale 8 | train_wall 144 | wall 2699
2021-03-05 12:44:13 | INFO | fairseq.trainer | begin training epoch 10
2021-03-05 12:44:29 | INFO | train_inner | epoch 010:     32 / 308 loss=6.808, nll_loss=5.499, ppl=45.22, wps=3674.4, ups=0.26, wpb=14196.5, bsz=556.1, num_updates=2800, lr=0.00035, gnorm=0.995, loss_scale=8, train_wall=47, wall=2716
2021-03-05 12:45:15 | INFO | train_inner | epoch 010:    132 / 308 loss=6.645, nll_loss=5.309, ppl=39.65, wps=30789.9, ups=2.19, wpb=14073, bsz=522.7, num_updates=2900, lr=0.0003625, gnorm=0.968, loss_scale=8, train_wall=45, wall=2762
2021-03-05 12:46:01 | INFO | train_inner | epoch 010:    232 / 308 loss=6.522, nll_loss=5.166, ppl=35.9, wps=30184.5, ups=2.14, wpb=14091.3, bsz=545.7, num_updates=3000, lr=0.000375, gnorm=0.938, loss_scale=8, train_wall=46, wall=2808
2021-03-05 12:46:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:46:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint10.pt (epoch 10 @ 3076 updates, score None) (writing took 6.454244936816394 seconds)
2021-03-05 12:46:44 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-03-05 12:46:44 | INFO | train | epoch 010 | loss 6.567 | nll_loss 5.218 | ppl 37.22 | wps 28798.3 | ups 2.04 | wpb 14123.4 | bsz 541.2 | num_updates 3076 | lr 0.0003845 | gnorm 0.964 | loss_scale 8 | train_wall 143 | wall 2851
2021-03-05 12:46:44 | INFO | fairseq.trainer | begin training epoch 11
2021-03-05 12:46:56 | INFO | train_inner | epoch 011:     24 / 308 loss=6.449, nll_loss=5.081, ppl=33.84, wps=25810.9, ups=1.82, wpb=14195.4, bsz=541.7, num_updates=3100, lr=0.0003875, gnorm=0.963, loss_scale=8, train_wall=47, wall=2863
2021-03-05 12:47:44 | INFO | train_inner | epoch 011:    124 / 308 loss=6.257, nll_loss=4.859, ppl=29.02, wps=29946.8, ups=2.12, wpb=14157.2, bsz=533, num_updates=3200, lr=0.0004, gnorm=0.865, loss_scale=8, train_wall=47, wall=2911
2021-03-05 12:48:30 | INFO | train_inner | epoch 011:    224 / 308 loss=6.199, nll_loss=4.791, ppl=27.68, wps=30536.6, ups=2.16, wpb=14108, bsz=560.7, num_updates=3300, lr=0.0004125, gnorm=0.901, loss_scale=8, train_wall=46, wall=2957
2021-03-05 12:49:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:49:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint11.pt (epoch 11 @ 3384 updates, score None) (writing took 6.588987016119063 seconds)
2021-03-05 12:49:16 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-03-05 12:49:16 | INFO | train | epoch 011 | loss 6.218 | nll_loss 4.813 | ppl 28.11 | wps 28629 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 3384 | lr 0.000423 | gnorm 0.891 | loss_scale 8 | train_wall 143 | wall 3002
2021-03-05 12:49:16 | INFO | fairseq.trainer | begin training epoch 12
2021-03-05 12:49:24 | INFO | train_inner | epoch 012:     16 / 308 loss=6.126, nll_loss=4.706, ppl=26.1, wps=25970, ups=1.85, wpb=14055, bsz=532.7, num_updates=3400, lr=0.000425, gnorm=0.895, loss_scale=8, train_wall=46, wall=3011
2021-03-05 12:50:11 | INFO | train_inner | epoch 012:    116 / 308 loss=5.966, nll_loss=4.523, ppl=22.99, wps=30311.2, ups=2.14, wpb=14187.2, bsz=550.6, num_updates=3500, lr=0.0004375, gnorm=0.869, loss_scale=8, train_wall=47, wall=3058
2021-03-05 12:50:58 | INFO | train_inner | epoch 012:    216 / 308 loss=5.97, nll_loss=4.524, ppl=23.01, wps=29751.5, ups=2.1, wpb=14164, bsz=529.5, num_updates=3600, lr=0.00045, gnorm=0.817, loss_scale=8, train_wall=47, wall=3105
2021-03-05 12:51:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 12:54:52 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.852 | nll_loss 4.262 | ppl 19.18 | bleu 7.22 | wps 3511.5 | wpb 12732.6 | bsz 485.9 | num_updates 3692 | best_bleu 7.22
2021-03-05 12:54:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:55:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint12.pt (epoch 12 @ 3692 updates, score 7.22) (writing took 11.039629804901779 seconds)
2021-03-05 12:55:03 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-03-05 12:55:03 | INFO | train | epoch 012 | loss 5.947 | nll_loss 4.499 | ppl 22.6 | wps 12506.6 | ups 0.89 | wpb 14123.4 | bsz 541.2 | num_updates 3692 | lr 0.0004615 | gnorm 0.837 | loss_scale 8 | train_wall 144 | wall 3350
2021-03-05 12:55:03 | INFO | fairseq.trainer | begin training epoch 13
2021-03-05 12:55:08 | INFO | train_inner | epoch 013:      8 / 308 loss=5.904, nll_loss=4.449, ppl=21.84, wps=5623.2, ups=0.4, wpb=14013.5, bsz=534.8, num_updates=3700, lr=0.0004625, gnorm=0.835, loss_scale=8, train_wall=46, wall=3354
2021-03-05 12:55:46 | INFO | train_inner | epoch 013:    108 / 308 loss=5.734, nll_loss=4.254, ppl=19.08, wps=37186.3, ups=2.63, wpb=14156.8, bsz=530.3, num_updates=3800, lr=0.000475, gnorm=0.8, loss_scale=8, train_wall=38, wall=3393
2021-03-05 12:56:24 | INFO | train_inner | epoch 013:    208 / 308 loss=5.725, nll_loss=4.243, ppl=18.93, wps=36621.5, ups=2.6, wpb=14077.5, bsz=557.3, num_updates=3900, lr=0.0004875, gnorm=0.799, loss_scale=8, train_wall=38, wall=3431
2021-03-05 12:57:06 | INFO | train_inner | epoch 013:    308 / 308 loss=5.702, nll_loss=4.216, ppl=18.58, wps=33737.6, ups=2.38, wpb=14158.7, bsz=542.5, num_updates=4000, lr=0.0005, gnorm=0.789, loss_scale=8, train_wall=42, wall=3473
2021-03-05 12:57:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:57:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint13.pt (epoch 13 @ 4000 updates, score None) (writing took 6.906253489665687 seconds)
2021-03-05 12:57:13 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-03-05 12:57:13 | INFO | train | epoch 013 | loss 5.725 | nll_loss 4.243 | ppl 18.93 | wps 33566.8 | ups 2.38 | wpb 14123.4 | bsz 541.2 | num_updates 4000 | lr 0.0005 | gnorm 0.799 | loss_scale 8 | train_wall 121 | wall 3480
2021-03-05 12:57:13 | INFO | fairseq.trainer | begin training epoch 14
2021-03-05 12:58:01 | INFO | train_inner | epoch 014:    100 / 308 loss=5.524, nll_loss=4.013, ppl=16.14, wps=25558.7, ups=1.81, wpb=14103.1, bsz=531.7, num_updates=4100, lr=0.000493865, gnorm=0.769, loss_scale=8, train_wall=47, wall=3528
2021-03-05 12:58:48 | INFO | train_inner | epoch 014:    200 / 308 loss=5.542, nll_loss=4.032, ppl=16.35, wps=30074, ups=2.13, wpb=14093.4, bsz=537.8, num_updates=4200, lr=0.00048795, gnorm=0.758, loss_scale=8, train_wall=47, wall=3575
2021-03-05 12:59:35 | INFO | train_inner | epoch 014:    300 / 308 loss=5.528, nll_loss=4.016, ppl=16.18, wps=30203.1, ups=2.13, wpb=14146.1, bsz=546.9, num_updates=4300, lr=0.000482243, gnorm=0.746, loss_scale=8, train_wall=47, wall=3622
2021-03-05 12:59:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 12:59:46 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint14.pt (epoch 14 @ 4308 updates, score None) (writing took 6.803644345141947 seconds)
2021-03-05 12:59:46 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-03-05 12:59:46 | INFO | train | epoch 014 | loss 5.532 | nll_loss 4.021 | ppl 16.24 | wps 28475.9 | ups 2.02 | wpb 14123.4 | bsz 541.2 | num_updates 4308 | lr 0.000481795 | gnorm 0.758 | loss_scale 8 | train_wall 144 | wall 3633
2021-03-05 12:59:46 | INFO | fairseq.trainer | begin training epoch 15
2021-03-05 13:00:30 | INFO | train_inner | epoch 015:     92 / 308 loss=5.37, nll_loss=3.837, ppl=14.29, wps=26157.6, ups=1.83, wpb=14273.6, bsz=552.1, num_updates=4400, lr=0.000476731, gnorm=0.731, loss_scale=8, train_wall=46, wall=3676
2021-03-05 13:01:15 | INFO | train_inner | epoch 015:    192 / 308 loss=5.347, nll_loss=3.81, ppl=14.02, wps=30748.8, ups=2.19, wpb=14010.9, bsz=550.5, num_updates=4500, lr=0.000471405, gnorm=0.727, loss_scale=8, train_wall=45, wall=3722
2021-03-05 13:02:02 | INFO | train_inner | epoch 015:    292 / 308 loss=5.37, nll_loss=3.835, ppl=14.27, wps=30053.8, ups=2.13, wpb=14078.8, bsz=527.9, num_updates=4600, lr=0.000466252, gnorm=0.725, loss_scale=8, train_wall=47, wall=3769
2021-03-05 13:02:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 13:07:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.483 | nll_loss 3.827 | ppl 14.19 | bleu 8.32 | wps 2086.8 | wpb 12732.6 | bsz 485.9 | num_updates 4616 | best_bleu 8.32
2021-03-05 13:07:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:07:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint15.pt (epoch 15 @ 4616 updates, score 8.32) (writing took 11.507175595033914 seconds)
2021-03-05 13:07:31 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-03-05 13:07:31 | INFO | train | epoch 015 | loss 5.356 | nll_loss 3.821 | ppl 14.13 | wps 9359.3 | ups 0.66 | wpb 14123.4 | bsz 541.2 | num_updates 4616 | lr 0.000465444 | gnorm 0.726 | loss_scale 8 | train_wall 142 | wall 4097
2021-03-05 13:07:31 | INFO | fairseq.trainer | begin training epoch 16
2021-03-05 13:08:11 | INFO | train_inner | epoch 016:     84 / 308 loss=5.205, nll_loss=3.649, ppl=12.54, wps=3823.8, ups=0.27, wpb=14106.3, bsz=552, num_updates=4700, lr=0.000461266, gnorm=0.711, loss_scale=8, train_wall=46, wall=4138
2021-03-05 13:08:57 | INFO | train_inner | epoch 016:    184 / 308 loss=5.215, nll_loss=3.658, ppl=12.62, wps=30456.4, ups=2.16, wpb=14083.1, bsz=539.6, num_updates=4800, lr=0.000456435, gnorm=0.721, loss_scale=8, train_wall=46, wall=4184
2021-03-05 13:09:44 | INFO | train_inner | epoch 016:    284 / 308 loss=5.221, nll_loss=3.666, ppl=12.69, wps=30126.5, ups=2.13, wpb=14167.3, bsz=539.2, num_updates=4900, lr=0.000451754, gnorm=0.714, loss_scale=8, train_wall=47, wall=4231
2021-03-05 13:09:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:10:02 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint16.pt (epoch 16 @ 4924 updates, score None) (writing took 6.825109166093171 seconds)
2021-03-05 13:10:02 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-03-05 13:10:02 | INFO | train | epoch 016 | loss 5.209 | nll_loss 3.652 | ppl 12.57 | wps 28762.2 | ups 2.04 | wpb 14123.4 | bsz 541.2 | num_updates 4924 | lr 0.000450652 | gnorm 0.713 | loss_scale 8 | train_wall 142 | wall 4249
2021-03-05 13:10:02 | INFO | fairseq.trainer | begin training epoch 17
2021-03-05 13:10:38 | INFO | train_inner | epoch 017:     76 / 308 loss=5.08, nll_loss=3.506, ppl=11.36, wps=26162.1, ups=1.85, wpb=14141.6, bsz=547.9, num_updates=5000, lr=0.000447214, gnorm=0.694, loss_scale=8, train_wall=46, wall=4285
2021-03-05 13:11:25 | INFO | train_inner | epoch 017:    176 / 308 loss=5.09, nll_loss=3.515, ppl=11.43, wps=30031.5, ups=2.14, wpb=14058.9, bsz=539.2, num_updates=5100, lr=0.000442807, gnorm=0.731, loss_scale=8, train_wall=47, wall=4332
2021-03-05 13:12:12 | INFO | train_inner | epoch 017:    276 / 308 loss=5.101, nll_loss=3.528, ppl=11.53, wps=30632.1, ups=2.14, wpb=14283.9, bsz=539, num_updates=5200, lr=0.000438529, gnorm=0.687, loss_scale=16, train_wall=46, wall=4379
2021-03-05 13:12:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:12:33 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint17.pt (epoch 17 @ 5232 updates, score None) (writing took 6.658558944705874 seconds)
2021-03-05 13:12:33 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-03-05 13:12:33 | INFO | train | epoch 017 | loss 5.085 | nll_loss 3.51 | ppl 11.39 | wps 28707.5 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 5232 | lr 0.000437186 | gnorm 0.706 | loss_scale 16 | train_wall 143 | wall 4400
2021-03-05 13:12:33 | INFO | fairseq.trainer | begin training epoch 18
2021-03-05 13:13:06 | INFO | train_inner | epoch 018:     68 / 308 loss=5.031, nll_loss=3.448, ppl=10.91, wps=26118.8, ups=1.85, wpb=14094.5, bsz=492.8, num_updates=5300, lr=0.000434372, gnorm=0.705, loss_scale=16, train_wall=46, wall=4433
2021-03-05 13:13:52 | INFO | train_inner | epoch 018:    168 / 308 loss=4.972, nll_loss=3.38, ppl=10.41, wps=30200.4, ups=2.14, wpb=14109.5, bsz=542.5, num_updates=5400, lr=0.000430331, gnorm=0.69, loss_scale=16, train_wall=47, wall=4479
2021-03-05 13:14:39 | INFO | train_inner | epoch 018:    268 / 308 loss=4.981, nll_loss=3.391, ppl=10.49, wps=30008.9, ups=2.13, wpb=14117, bsz=569.3, num_updates=5500, lr=0.000426401, gnorm=0.689, loss_scale=16, train_wall=47, wall=4526
2021-03-05 13:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 13:20:04 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.323 | nll_loss 3.638 | ppl 12.45 | bleu 9.53 | wps 2115.5 | wpb 12732.6 | bsz 485.9 | num_updates 5540 | best_bleu 9.53
2021-03-05 13:20:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:20:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint18.pt (epoch 18 @ 5540 updates, score 9.53) (writing took 11.4406555169262 seconds)
2021-03-05 13:20:16 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-03-05 13:20:16 | INFO | train | epoch 018 | loss 4.974 | nll_loss 3.383 | ppl 10.43 | wps 9404.4 | ups 0.67 | wpb 14123.4 | bsz 541.2 | num_updates 5540 | lr 0.000424859 | gnorm 0.691 | loss_scale 16 | train_wall 144 | wall 4863
2021-03-05 13:20:16 | INFO | fairseq.trainer | begin training epoch 19
2021-03-05 13:20:46 | INFO | train_inner | epoch 019:     60 / 308 loss=4.89, nll_loss=3.287, ppl=9.76, wps=3853.5, ups=0.27, wpb=14110.1, bsz=540.6, num_updates=5600, lr=0.000422577, gnorm=0.672, loss_scale=16, train_wall=48, wall=4893
2021-03-05 13:21:33 | INFO | train_inner | epoch 019:    160 / 308 loss=4.867, nll_loss=3.26, ppl=9.58, wps=29877.9, ups=2.11, wpb=14172.4, bsz=544.4, num_updates=5700, lr=0.000418854, gnorm=0.679, loss_scale=16, train_wall=47, wall=4940
2021-03-05 13:22:19 | INFO | train_inner | epoch 019:    260 / 308 loss=4.904, nll_loss=3.302, ppl=9.86, wps=30666.7, ups=2.17, wpb=14136.2, bsz=548.1, num_updates=5800, lr=0.000415227, gnorm=0.697, loss_scale=16, train_wall=46, wall=4986
2021-03-05 13:22:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:22:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint19.pt (epoch 19 @ 5848 updates, score None) (writing took 6.762022141832858 seconds)
2021-03-05 13:22:49 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-03-05 13:22:49 | INFO | train | epoch 019 | loss 4.877 | nll_loss 3.272 | ppl 9.66 | wps 28414.6 | ups 2.01 | wpb 14123.4 | bsz 541.2 | num_updates 5848 | lr 0.00041352 | gnorm 0.68 | loss_scale 16 | train_wall 144 | wall 5016
2021-03-05 13:22:49 | INFO | fairseq.trainer | begin training epoch 20
2021-03-05 13:23:15 | INFO | train_inner | epoch 020:     52 / 308 loss=4.84, nll_loss=3.23, ppl=9.38, wps=25281.5, ups=1.8, wpb=14041.7, bsz=533.5, num_updates=5900, lr=0.000411693, gnorm=0.679, loss_scale=16, train_wall=47, wall=5042
2021-03-05 13:24:01 | INFO | train_inner | epoch 020:    152 / 308 loss=4.781, nll_loss=3.161, ppl=8.95, wps=30633.1, ups=2.16, wpb=14173.9, bsz=544.7, num_updates=6000, lr=0.000408248, gnorm=0.687, loss_scale=16, train_wall=46, wall=5088
2021-03-05 13:24:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 13:29:09 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.251 | nll_loss 3.559 | ppl 11.79 | bleu 9.57 | wps 2098.3 | wpb 12732.6 | bsz 485.9 | num_updates 6000 | best_bleu 9.57
2021-03-05 13:29:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:29:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_20_6000.pt (epoch 20 @ 6000 updates, score 9.57) (writing took 11.62390752369538 seconds)
2021-03-05 13:30:08 | INFO | train_inner | epoch 020:    252 / 308 loss=4.796, nll_loss=3.178, ppl=9.05, wps=3870.7, ups=0.27, wpb=14189.2, bsz=538.2, num_updates=6100, lr=0.000404888, gnorm=0.694, loss_scale=16, train_wall=47, wall=5455
2021-03-05 13:30:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:30:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint20.pt (epoch 20 @ 6156 updates, score None) (writing took 6.661493564024568 seconds)
2021-03-05 13:30:41 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-03-05 13:30:41 | INFO | train | epoch 020 | loss 4.793 | nll_loss 3.176 | ppl 9.04 | wps 9214.1 | ups 0.65 | wpb 14123.4 | bsz 541.2 | num_updates 6156 | lr 0.000403042 | gnorm 0.69 | loss_scale 16 | train_wall 144 | wall 5488
2021-03-05 13:30:41 | INFO | fairseq.trainer | begin training epoch 21
2021-03-05 13:31:02 | INFO | train_inner | epoch 021:     44 / 308 loss=4.747, nll_loss=3.124, ppl=8.72, wps=25580.4, ups=1.83, wpb=14011.3, bsz=545, num_updates=6200, lr=0.00040161, gnorm=0.686, loss_scale=16, train_wall=47, wall=5509
2021-03-05 13:31:49 | INFO | train_inner | epoch 021:    144 / 308 loss=4.705, nll_loss=3.074, ppl=8.42, wps=30145.5, ups=2.13, wpb=14137.6, bsz=537.7, num_updates=6300, lr=0.00039841, gnorm=0.678, loss_scale=16, train_wall=47, wall=5556
2021-03-05 13:32:32 | INFO | train_inner | epoch 021:    244 / 308 loss=4.717, nll_loss=3.089, ppl=8.51, wps=32519.7, ups=2.32, wpb=14038.3, bsz=556.3, num_updates=6400, lr=0.000395285, gnorm=0.68, loss_scale=16, train_wall=43, wall=5599
2021-03-05 13:33:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 13:38:05 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.214 | nll_loss 3.521 | ppl 11.48 | bleu 10.08 | wps 2144.5 | wpb 12732.6 | bsz 485.9 | num_updates 6464 | best_bleu 10.08
2021-03-05 13:38:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:38:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint21.pt (epoch 21 @ 6464 updates, score 10.08) (writing took 11.562725238967687 seconds)
2021-03-05 13:38:16 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-03-05 13:38:16 | INFO | train | epoch 021 | loss 4.717 | nll_loss 3.089 | ppl 8.51 | wps 9560.7 | ups 0.68 | wpb 14123.4 | bsz 541.2 | num_updates 6464 | lr 0.000393323 | gnorm 0.679 | loss_scale 16 | train_wall 140 | wall 5943
2021-03-05 13:38:16 | INFO | fairseq.trainer | begin training epoch 22
2021-03-05 13:38:35 | INFO | train_inner | epoch 022:     36 / 308 loss=4.698, nll_loss=3.066, ppl=8.38, wps=3932.3, ups=0.28, wpb=14237.8, bsz=528.9, num_updates=6500, lr=0.000392232, gnorm=0.674, loss_scale=16, train_wall=47, wall=5961
2021-03-05 13:39:21 | INFO | train_inner | epoch 022:    136 / 308 loss=4.612, nll_loss=2.968, ppl=7.82, wps=30150.5, ups=2.13, wpb=14123.4, bsz=559.4, num_updates=6600, lr=0.000389249, gnorm=0.669, loss_scale=16, train_wall=47, wall=6008
2021-03-05 13:40:07 | INFO | train_inner | epoch 022:    236 / 308 loss=4.667, nll_loss=3.03, ppl=8.17, wps=30471.3, ups=2.18, wpb=14002.5, bsz=532.1, num_updates=6700, lr=0.000386334, gnorm=0.677, loss_scale=16, train_wall=46, wall=6054
2021-03-05 13:40:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:40:48 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint22.pt (epoch 22 @ 6772 updates, score None) (writing took 6.814205984584987 seconds)
2021-03-05 13:40:48 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-03-05 13:40:48 | INFO | train | epoch 022 | loss 4.644 | nll_loss 3.004 | ppl 8.02 | wps 28609.3 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 6772 | lr 0.000384274 | gnorm 0.676 | loss_scale 16 | train_wall 143 | wall 6095
2021-03-05 13:40:48 | INFO | fairseq.trainer | begin training epoch 23
2021-03-05 13:41:03 | INFO | train_inner | epoch 023:     28 / 308 loss=4.63, nll_loss=2.989, ppl=7.94, wps=25829.6, ups=1.81, wpb=14263.1, bsz=549.5, num_updates=6800, lr=0.000383482, gnorm=0.683, loss_scale=16, train_wall=47, wall=6110
2021-03-05 13:41:50 | INFO | train_inner | epoch 023:    128 / 308 loss=4.58, nll_loss=2.929, ppl=7.62, wps=29952.9, ups=2.12, wpb=14095.2, bsz=508.8, num_updates=6900, lr=0.000380693, gnorm=0.68, loss_scale=16, train_wall=47, wall=6157
2021-03-05 13:42:36 | INFO | train_inner | epoch 023:    228 / 308 loss=4.568, nll_loss=2.917, ppl=7.55, wps=30104.6, ups=2.15, wpb=14012.5, bsz=546, num_updates=7000, lr=0.000377964, gnorm=0.679, loss_scale=16, train_wall=46, wall=6203
2021-03-05 13:43:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:43:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint23.pt (epoch 23 @ 7080 updates, score None) (writing took 6.48571084626019 seconds)
2021-03-05 13:43:20 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-03-05 13:43:20 | INFO | train | epoch 023 | loss 4.579 | nll_loss 2.929 | ppl 7.62 | wps 28719.2 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 7080 | lr 0.000375823 | gnorm 0.681 | loss_scale 16 | train_wall 143 | wall 6247
2021-03-05 13:43:20 | INFO | fairseq.trainer | begin training epoch 24
2021-03-05 13:43:30 | INFO | train_inner | epoch 024:     20 / 308 loss=4.596, nll_loss=2.949, ppl=7.72, wps=26073.1, ups=1.84, wpb=14151.8, bsz=551.6, num_updates=7100, lr=0.000375293, gnorm=0.681, loss_scale=16, train_wall=46, wall=6257
2021-03-05 13:44:17 | INFO | train_inner | epoch 024:    120 / 308 loss=4.481, nll_loss=2.816, ppl=7.04, wps=30667.8, ups=2.16, wpb=14214.4, bsz=530.1, num_updates=7200, lr=0.000372678, gnorm=0.673, loss_scale=16, train_wall=46, wall=6304
2021-03-05 13:45:04 | INFO | train_inner | epoch 024:    220 / 308 loss=4.535, nll_loss=2.878, ppl=7.35, wps=29880.2, ups=2.13, wpb=14041.2, bsz=549.5, num_updates=7300, lr=0.000370117, gnorm=0.696, loss_scale=16, train_wall=47, wall=6351
2021-03-05 13:45:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 13:50:53 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.145 | nll_loss 3.444 | ppl 10.88 | bleu 9.98 | wps 2097.7 | wpb 12732.6 | bsz 485.9 | num_updates 7388 | best_bleu 10.08
2021-03-05 13:50:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:51:00 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint24.pt (epoch 24 @ 7388 updates, score 9.98) (writing took 6.879524826072156 seconds)
2021-03-05 13:51:00 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-03-05 13:51:00 | INFO | train | epoch 024 | loss 4.521 | nll_loss 2.863 | ppl 7.27 | wps 9452.3 | ups 0.67 | wpb 14123.4 | bsz 541.2 | num_updates 7388 | lr 0.000367906 | gnorm 0.68 | loss_scale 16 | train_wall 144 | wall 6707
2021-03-05 13:51:00 | INFO | fairseq.trainer | begin training epoch 25
2021-03-05 13:51:07 | INFO | train_inner | epoch 025:     12 / 308 loss=4.544, nll_loss=2.889, ppl=7.41, wps=3909.2, ups=0.28, wpb=14184.4, bsz=546.4, num_updates=7400, lr=0.000367607, gnorm=0.675, loss_scale=16, train_wall=47, wall=6714
2021-03-05 13:51:54 | INFO | train_inner | epoch 025:    112 / 308 loss=4.415, nll_loss=2.741, ppl=6.69, wps=29641.9, ups=2.1, wpb=14134.9, bsz=544.2, num_updates=7500, lr=0.000365148, gnorm=0.665, loss_scale=16, train_wall=47, wall=6761
2021-03-05 13:52:41 | INFO | train_inner | epoch 025:    212 / 308 loss=4.492, nll_loss=2.828, ppl=7.1, wps=30315.7, ups=2.16, wpb=14062, bsz=525.1, num_updates=7600, lr=0.000362738, gnorm=0.695, loss_scale=16, train_wall=46, wall=6808
2021-03-05 13:53:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:53:33 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint25.pt (epoch 25 @ 7696 updates, score None) (writing took 6.548934542573988 seconds)
2021-03-05 13:53:33 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-03-05 13:53:33 | INFO | train | epoch 025 | loss 4.465 | nll_loss 2.798 | ppl 6.95 | wps 28454.2 | ups 2.01 | wpb 14123.4 | bsz 541.2 | num_updates 7696 | lr 0.000360469 | gnorm 0.678 | loss_scale 16 | train_wall 144 | wall 6860
2021-03-05 13:53:33 | INFO | fairseq.trainer | begin training epoch 26
2021-03-05 13:53:36 | INFO | train_inner | epoch 026:      4 / 308 loss=4.496, nll_loss=2.833, ppl=7.13, wps=25701.4, ups=1.81, wpb=14176.1, bsz=550.6, num_updates=7700, lr=0.000360375, gnorm=0.675, loss_scale=16, train_wall=47, wall=6863
2021-03-05 13:54:13 | INFO | train_inner | epoch 026:    104 / 308 loss=4.345, nll_loss=2.659, ppl=6.32, wps=37887.1, ups=2.69, wpb=14059.1, bsz=551.5, num_updates=7800, lr=0.000358057, gnorm=0.665, loss_scale=16, train_wall=37, wall=6900
2021-03-05 13:54:50 | INFO | train_inner | epoch 026:    204 / 308 loss=4.426, nll_loss=2.752, ppl=6.74, wps=38115.5, ups=2.67, wpb=14260.1, bsz=543.3, num_updates=7900, lr=0.000355784, gnorm=0.677, loss_scale=16, train_wall=37, wall=6937
2021-03-05 13:55:29 | INFO | train_inner | epoch 026:    304 / 308 loss=4.458, nll_loss=2.79, ppl=6.91, wps=36288.4, ups=2.59, wpb=14017.5, bsz=537.3, num_updates=8000, lr=0.000353553, gnorm=0.685, loss_scale=16, train_wall=38, wall=6976
2021-03-05 13:55:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 13:58:17 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.137 | nll_loss 3.432 | ppl 10.79 | bleu 10.15 | wps 3695.7 | wpb 12732.6 | bsz 485.9 | num_updates 8000 | best_bleu 10.15
2021-03-05 13:58:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:58:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_26_8000.pt (epoch 26 @ 8000 updates, score 10.15) (writing took 11.227837523911148 seconds)
2021-03-05 13:58:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 13:58:37 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint26.pt (epoch 26 @ 8004 updates, score None) (writing took 6.569456413388252 seconds)
2021-03-05 13:58:37 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-03-05 13:58:37 | INFO | train | epoch 026 | loss 4.412 | nll_loss 2.736 | ppl 6.66 | wps 14291.3 | ups 1.01 | wpb 14123.4 | bsz 541.2 | num_updates 8004 | lr 0.000353465 | gnorm 0.675 | loss_scale 16 | train_wall 116 | wall 7164
2021-03-05 13:58:37 | INFO | fairseq.trainer | begin training epoch 27
2021-03-05 13:59:23 | INFO | train_inner | epoch 027:     96 / 308 loss=4.334, nll_loss=2.645, ppl=6.26, wps=6093.9, ups=0.43, wpb=14250.6, bsz=532.8, num_updates=8100, lr=0.000351364, gnorm=0.678, loss_scale=16, train_wall=46, wall=7210
2021-03-05 14:00:10 | INFO | train_inner | epoch 027:    196 / 308 loss=4.365, nll_loss=2.682, ppl=6.42, wps=29630.9, ups=2.11, wpb=14048.1, bsz=541.9, num_updates=8200, lr=0.000349215, gnorm=0.682, loss_scale=16, train_wall=47, wall=7257
2021-03-05 14:00:57 | INFO | train_inner | epoch 027:    296 / 308 loss=4.398, nll_loss=2.721, ppl=6.59, wps=30237.3, ups=2.15, wpb=14081.1, bsz=542.7, num_updates=8300, lr=0.000347105, gnorm=0.686, loss_scale=16, train_wall=46, wall=7304
2021-03-05 14:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 14:06:06 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.143 | nll_loss 3.434 | ppl 10.81 | bleu 10.52 | wps 2127.6 | wpb 12732.6 | bsz 485.9 | num_updates 8312 | best_bleu 10.52
2021-03-05 14:06:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:06:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint27.pt (epoch 27 @ 8312 updates, score 10.52) (writing took 11.618172721005976 seconds)
2021-03-05 14:06:18 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-03-05 14:06:18 | INFO | train | epoch 027 | loss 4.363 | nll_loss 2.68 | ppl 6.41 | wps 9442.3 | ups 0.67 | wpb 14123.4 | bsz 541.2 | num_updates 8312 | lr 0.000346854 | gnorm 0.681 | loss_scale 16 | train_wall 143 | wall 7625
2021-03-05 14:06:18 | INFO | fairseq.trainer | begin training epoch 28
2021-03-05 14:07:01 | INFO | train_inner | epoch 028:     88 / 308 loss=4.275, nll_loss=2.578, ppl=5.97, wps=3887.1, ups=0.27, wpb=14137.9, bsz=545.5, num_updates=8400, lr=0.000345033, gnorm=0.68, loss_scale=16, train_wall=47, wall=7668
2021-03-05 14:07:47 | INFO | train_inner | epoch 028:    188 / 308 loss=4.319, nll_loss=2.628, ppl=6.18, wps=30375.5, ups=2.15, wpb=14115.9, bsz=542.6, num_updates=8500, lr=0.000342997, gnorm=0.688, loss_scale=16, train_wall=46, wall=7714
2021-03-05 14:08:34 | INFO | train_inner | epoch 028:    288 / 308 loss=4.352, nll_loss=2.667, ppl=6.35, wps=29860.1, ups=2.12, wpb=14070.6, bsz=532.7, num_updates=8600, lr=0.000340997, gnorm=0.692, loss_scale=16, train_wall=47, wall=7761
2021-03-05 14:08:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:08:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint28.pt (epoch 28 @ 8620 updates, score None) (writing took 6.712925280909985 seconds)
2021-03-05 14:08:51 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-03-05 14:08:51 | INFO | train | epoch 028 | loss 4.317 | nll_loss 2.627 | ppl 6.18 | wps 28444.3 | ups 2.01 | wpb 14123.4 | bsz 541.2 | num_updates 8620 | lr 0.000340601 | gnorm 0.687 | loss_scale 16 | train_wall 144 | wall 7778
2021-03-05 14:08:51 | INFO | fairseq.trainer | begin training epoch 29
2021-03-05 14:09:30 | INFO | train_inner | epoch 029:     80 / 308 loss=4.261, nll_loss=2.562, ppl=5.9, wps=25521.5, ups=1.81, wpb=14124, bsz=545.2, num_updates=8700, lr=0.000339032, gnorm=0.677, loss_scale=16, train_wall=47, wall=7817
2021-03-05 14:10:16 | INFO | train_inner | epoch 029:    180 / 308 loss=4.261, nll_loss=2.562, ppl=5.9, wps=30372, ups=2.14, wpb=14220.2, bsz=545.2, num_updates=8800, lr=0.0003371, gnorm=0.672, loss_scale=16, train_wall=47, wall=7863
2021-03-05 14:11:03 | INFO | train_inner | epoch 029:    280 / 308 loss=4.302, nll_loss=2.609, ppl=6.1, wps=29983.5, ups=2.13, wpb=14063.5, bsz=548.3, num_updates=8900, lr=0.000335201, gnorm=0.689, loss_scale=16, train_wall=47, wall=7910
2021-03-05 14:11:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:11:23 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint29.pt (epoch 29 @ 8928 updates, score None) (writing took 6.554623096249998 seconds)
2021-03-05 14:11:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-03-05 14:11:23 | INFO | train | epoch 029 | loss 4.276 | nll_loss 2.578 | ppl 5.97 | wps 28639 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 8928 | lr 0.000334675 | gnorm 0.682 | loss_scale 16 | train_wall 143 | wall 7930
2021-03-05 14:11:23 | INFO | fairseq.trainer | begin training epoch 30
2021-03-05 14:11:58 | INFO | train_inner | epoch 030:     72 / 308 loss=4.233, nll_loss=2.528, ppl=5.77, wps=25967.8, ups=1.84, wpb=14102.4, bsz=529.9, num_updates=9000, lr=0.000333333, gnorm=0.696, loss_scale=16, train_wall=46, wall=7965
2021-03-05 14:12:45 | INFO | train_inner | epoch 030:    172 / 308 loss=4.221, nll_loss=2.515, ppl=5.71, wps=30456.6, ups=2.13, wpb=14323.9, bsz=548.9, num_updates=9100, lr=0.000331497, gnorm=0.68, loss_scale=16, train_wall=47, wall=8012
2021-03-05 14:13:31 | INFO | train_inner | epoch 030:    272 / 308 loss=4.261, nll_loss=2.562, ppl=5.9, wps=30068.6, ups=2.14, wpb=14029.8, bsz=534.7, num_updates=9200, lr=0.00032969, gnorm=0.693, loss_scale=16, train_wall=46, wall=8058
2021-03-05 14:13:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 14:18:57 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.118 | nll_loss 3.411 | ppl 10.64 | bleu 10.51 | wps 2092.8 | wpb 12732.6 | bsz 485.9 | num_updates 9236 | best_bleu 10.52
2021-03-05 14:18:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:19:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint30.pt (epoch 30 @ 9236 updates, score 10.51) (writing took 6.588118067011237 seconds)
2021-03-05 14:19:03 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-03-05 14:19:03 | INFO | train | epoch 030 | loss 4.234 | nll_loss 2.53 | ppl 5.78 | wps 9447.9 | ups 0.67 | wpb 14123.4 | bsz 541.2 | num_updates 9236 | lr 0.000329047 | gnorm 0.691 | loss_scale 16 | train_wall 143 | wall 8390
2021-03-05 14:19:03 | INFO | fairseq.trainer | begin training epoch 31
2021-03-05 14:19:35 | INFO | train_inner | epoch 031:     64 / 308 loss=4.208, nll_loss=2.499, ppl=5.65, wps=3889.4, ups=0.28, wpb=14125.6, bsz=544.8, num_updates=9300, lr=0.000327913, gnorm=0.692, loss_scale=32, train_wall=47, wall=8422
2021-03-05 14:20:22 | INFO | train_inner | epoch 031:    164 / 308 loss=4.171, nll_loss=2.456, ppl=5.49, wps=29768.3, ups=2.11, wpb=14084.7, bsz=529, num_updates=9400, lr=0.000326164, gnorm=0.687, loss_scale=32, train_wall=47, wall=8469
2021-03-05 14:21:09 | INFO | train_inner | epoch 031:    264 / 308 loss=4.226, nll_loss=2.52, ppl=5.74, wps=29821.2, ups=2.13, wpb=14006.3, bsz=543.9, num_updates=9500, lr=0.000324443, gnorm=0.703, loss_scale=32, train_wall=47, wall=8516
2021-03-05 14:21:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:21:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint31.pt (epoch 31 @ 9544 updates, score None) (writing took 6.8244585720822215 seconds)
2021-03-05 14:21:36 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-03-05 14:21:36 | INFO | train | epoch 031 | loss 4.195 | nll_loss 2.484 | ppl 5.6 | wps 28405.2 | ups 2.01 | wpb 14123.4 | bsz 541.2 | num_updates 9544 | lr 0.000323694 | gnorm 0.689 | loss_scale 32 | train_wall 144 | wall 8543
2021-03-05 14:21:36 | INFO | fairseq.trainer | begin training epoch 32
2021-03-05 14:22:04 | INFO | train_inner | epoch 032:     56 / 308 loss=4.167, nll_loss=2.453, ppl=5.47, wps=25823, ups=1.82, wpb=14213.9, bsz=539.6, num_updates=9600, lr=0.000322749, gnorm=0.676, loss_scale=32, train_wall=47, wall=8571
2021-03-05 14:22:51 | INFO | train_inner | epoch 032:    156 / 308 loss=4.135, nll_loss=2.414, ppl=5.33, wps=30042.5, ups=2.13, wpb=14099, bsz=539.4, num_updates=9700, lr=0.000321081, gnorm=0.694, loss_scale=32, train_wall=47, wall=8618
2021-03-05 14:23:38 | INFO | train_inner | epoch 032:    256 / 308 loss=4.196, nll_loss=2.484, ppl=5.6, wps=30041.7, ups=2.13, wpb=14116.8, bsz=527.3, num_updates=9800, lr=0.000319438, gnorm=0.702, loss_scale=32, train_wall=47, wall=8665
2021-03-05 14:24:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:24:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint32.pt (epoch 32 @ 9852 updates, score None) (writing took 6.843416444025934 seconds)
2021-03-05 14:24:09 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-03-05 14:24:09 | INFO | train | epoch 032 | loss 4.158 | nll_loss 2.441 | ppl 5.43 | wps 28473.9 | ups 2.02 | wpb 14123.4 | bsz 541.2 | num_updates 9852 | lr 0.000318594 | gnorm 0.692 | loss_scale 32 | train_wall 144 | wall 8696
2021-03-05 14:24:09 | INFO | fairseq.trainer | begin training epoch 33
2021-03-05 14:24:33 | INFO | train_inner | epoch 033:     48 / 308 loss=4.122, nll_loss=2.4, ppl=5.28, wps=25867.4, ups=1.83, wpb=14157.6, bsz=555.5, num_updates=9900, lr=0.000317821, gnorm=0.684, loss_scale=32, train_wall=46, wall=8720
2021-03-05 14:25:19 | INFO | train_inner | epoch 033:    148 / 308 loss=4.108, nll_loss=2.383, ppl=5.22, wps=30422.9, ups=2.16, wpb=14098.8, bsz=534.1, num_updates=10000, lr=0.000316228, gnorm=0.706, loss_scale=32, train_wall=46, wall=8766
2021-03-05 14:25:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 14:30:23 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.154 | nll_loss 3.441 | ppl 10.86 | bleu 10.63 | wps 2125.3 | wpb 12732.6 | bsz 485.9 | num_updates 10000 | best_bleu 10.63
2021-03-05 14:30:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:30:35 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_33_10000.pt (epoch 33 @ 10000 updates, score 10.63) (writing took 11.966243727132678 seconds)
2021-03-05 14:31:22 | INFO | train_inner | epoch 033:    248 / 308 loss=4.132, nll_loss=2.412, ppl=5.32, wps=3910.8, ups=0.28, wpb=14182.9, bsz=560.2, num_updates=10100, lr=0.000314658, gnorm=0.691, loss_scale=32, train_wall=46, wall=9129
2021-03-05 14:31:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 14:36:58 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.143 | nll_loss 3.427 | ppl 10.75 | bleu 10.86 | wps 2091.1 | wpb 12732.6 | bsz 485.9 | num_updates 10160 | best_bleu 10.86
2021-03-05 14:36:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:37:10 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint33.pt (epoch 33 @ 10160 updates, score 10.86) (writing took 11.872638538014144 seconds)
2021-03-05 14:37:10 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-03-05 14:37:10 | INFO | train | epoch 033 | loss 4.123 | nll_loss 2.4 | ppl 5.28 | wps 5569.7 | ups 0.39 | wpb 14123.4 | bsz 541.2 | num_updates 10160 | lr 0.000313728 | gnorm 0.697 | loss_scale 32 | train_wall 143 | wall 9477
2021-03-05 14:37:10 | INFO | fairseq.trainer | begin training epoch 34
2021-03-05 14:37:30 | INFO | train_inner | epoch 034:     40 / 308 loss=4.115, nll_loss=2.391, ppl=5.25, wps=3822.1, ups=0.27, wpb=14094.9, bsz=540.5, num_updates=10200, lr=0.000313112, gnorm=0.689, loss_scale=32, train_wall=47, wall=9497
2021-03-05 14:38:17 | INFO | train_inner | epoch 034:    140 / 308 loss=4.056, nll_loss=2.322, ppl=5, wps=30336.7, ups=2.16, wpb=14047.7, bsz=534.5, num_updates=10300, lr=0.000311588, gnorm=0.694, loss_scale=32, train_wall=46, wall=9544
2021-03-05 14:39:05 | INFO | train_inner | epoch 034:    240 / 308 loss=4.117, nll_loss=2.393, ppl=5.25, wps=29779.5, ups=2.09, wpb=14230, bsz=554.7, num_updates=10400, lr=0.000310087, gnorm=0.697, loss_scale=32, train_wall=48, wall=9591
2021-03-05 14:39:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:39:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint34.pt (epoch 34 @ 10468 updates, score None) (writing took 6.965998156927526 seconds)
2021-03-05 14:39:44 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-03-05 14:39:44 | INFO | train | epoch 034 | loss 4.089 | nll_loss 2.361 | ppl 5.14 | wps 28326 | ups 2.01 | wpb 14123.4 | bsz 541.2 | num_updates 10468 | lr 0.000309078 | gnorm 0.695 | loss_scale 32 | train_wall 145 | wall 9631
2021-03-05 14:39:44 | INFO | fairseq.trainer | begin training epoch 35
2021-03-05 14:40:00 | INFO | train_inner | epoch 035:     32 / 308 loss=4.082, nll_loss=2.353, ppl=5.11, wps=25403.8, ups=1.8, wpb=14119.4, bsz=536.4, num_updates=10500, lr=0.000308607, gnorm=0.702, loss_scale=32, train_wall=47, wall=9647
2021-03-05 14:40:47 | INFO | train_inner | epoch 035:    132 / 308 loss=4.017, nll_loss=2.277, ppl=4.85, wps=30058.1, ups=2.14, wpb=14046.6, bsz=530.6, num_updates=10600, lr=0.000307148, gnorm=0.7, loss_scale=32, train_wall=47, wall=9694
2021-03-05 14:41:34 | INFO | train_inner | epoch 035:    232 / 308 loss=4.078, nll_loss=2.347, ppl=5.09, wps=30303.6, ups=2.14, wpb=14178, bsz=559.1, num_updates=10700, lr=0.000305709, gnorm=0.696, loss_scale=32, train_wall=47, wall=9741
2021-03-05 14:42:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:42:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint35.pt (epoch 35 @ 10776 updates, score None) (writing took 6.82684459304437 seconds)
2021-03-05 14:42:16 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-03-05 14:42:16 | INFO | train | epoch 035 | loss 4.056 | nll_loss 2.322 | ppl 5 | wps 28509.6 | ups 2.02 | wpb 14123.4 | bsz 541.2 | num_updates 10776 | lr 0.000304629 | gnorm 0.698 | loss_scale 32 | train_wall 144 | wall 9783
2021-03-05 14:42:16 | INFO | fairseq.trainer | begin training epoch 36
2021-03-05 14:42:29 | INFO | train_inner | epoch 036:     24 / 308 loss=4.076, nll_loss=2.345, ppl=5.08, wps=25587.8, ups=1.81, wpb=14122.9, bsz=532.1, num_updates=10800, lr=0.00030429, gnorm=0.694, loss_scale=32, train_wall=47, wall=9796
2021-03-05 14:43:16 | INFO | train_inner | epoch 036:    124 / 308 loss=3.979, nll_loss=2.233, ppl=4.7, wps=30203.3, ups=2.14, wpb=14111.9, bsz=558.7, num_updates=10900, lr=0.000302891, gnorm=0.701, loss_scale=32, train_wall=47, wall=9842
2021-03-05 14:44:03 | INFO | train_inner | epoch 036:    224 / 308 loss=4.054, nll_loss=2.319, ppl=4.99, wps=30097.4, ups=2.13, wpb=14136.1, bsz=527.2, num_updates=11000, lr=0.000301511, gnorm=0.704, loss_scale=32, train_wall=47, wall=9889
2021-03-05 14:44:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 14:49:52 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.146 | nll_loss 3.432 | ppl 10.79 | bleu 10.64 | wps 2082.1 | wpb 12732.6 | bsz 485.9 | num_updates 11084 | best_bleu 10.86
2021-03-05 14:49:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:49:59 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint36.pt (epoch 36 @ 11084 updates, score 10.64) (writing took 6.800222720019519 seconds)
2021-03-05 14:49:59 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-03-05 14:49:59 | INFO | train | epoch 036 | loss 4.027 | nll_loss 2.288 | ppl 4.88 | wps 9396.2 | ups 0.67 | wpb 14123.4 | bsz 541.2 | num_updates 11084 | lr 0.000300367 | gnorm 0.7 | loss_scale 32 | train_wall 144 | wall 10246
2021-03-05 14:49:59 | INFO | fairseq.trainer | begin training epoch 37
2021-03-05 14:50:08 | INFO | train_inner | epoch 037:     16 / 308 loss=4.054, nll_loss=2.319, ppl=4.99, wps=3852.2, ups=0.27, wpb=14064.6, bsz=524.9, num_updates=11100, lr=0.00030015, gnorm=0.706, loss_scale=32, train_wall=46, wall=10255
2021-03-05 14:50:55 | INFO | train_inner | epoch 037:    116 / 308 loss=3.968, nll_loss=2.218, ppl=4.65, wps=29772.2, ups=2.11, wpb=14099.9, bsz=541, num_updates=11200, lr=0.000298807, gnorm=0.704, loss_scale=32, train_wall=47, wall=10302
2021-03-05 14:51:41 | INFO | train_inner | epoch 037:    216 / 308 loss=3.997, nll_loss=2.253, ppl=4.77, wps=30568.6, ups=2.16, wpb=14159.1, bsz=542.5, num_updates=11300, lr=0.000297482, gnorm=0.706, loss_scale=32, train_wall=46, wall=10348
2021-03-05 14:52:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:52:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint37.pt (epoch 37 @ 11392 updates, score None) (writing took 7.052579591982067 seconds)
2021-03-05 14:52:31 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-03-05 14:52:31 | INFO | train | epoch 037 | loss 3.997 | nll_loss 2.253 | ppl 4.77 | wps 28651.1 | ups 2.03 | wpb 14123.4 | bsz 541.2 | num_updates 11392 | lr 0.000296278 | gnorm 0.706 | loss_scale 32 | train_wall 143 | wall 10398
2021-03-05 14:52:31 | INFO | fairseq.trainer | begin training epoch 38
2021-03-05 14:52:36 | INFO | train_inner | epoch 038:      8 / 308 loss=4.029, nll_loss=2.291, ppl=4.89, wps=25829.8, ups=1.82, wpb=14166.1, bsz=550.1, num_updates=11400, lr=0.000296174, gnorm=0.709, loss_scale=32, train_wall=46, wall=10403
2021-03-05 14:53:23 | INFO | train_inner | epoch 038:    108 / 308 loss=3.912, nll_loss=2.154, ppl=4.45, wps=30132.4, ups=2.14, wpb=14102, bsz=554.8, num_updates=11500, lr=0.000294884, gnorm=0.7, loss_scale=32, train_wall=47, wall=10450
2021-03-05 14:54:08 | INFO | train_inner | epoch 038:    208 / 308 loss=3.976, nll_loss=2.228, ppl=4.68, wps=31407.5, ups=2.22, wpb=14168.4, bsz=533, num_updates=11600, lr=0.00029361, gnorm=0.705, loss_scale=32, train_wall=45, wall=10495
2021-03-05 14:54:47 | INFO | train_inner | epoch 038:    308 / 308 loss=4.024, nll_loss=2.284, ppl=4.87, wps=36340.1, ups=2.58, wpb=14088.9, bsz=535.6, num_updates=11700, lr=0.000292353, gnorm=0.721, loss_scale=32, train_wall=39, wall=10534
2021-03-05 14:54:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 14:54:53 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint38.pt (epoch 38 @ 11700 updates, score None) (writing took 6.287776997312903 seconds)
2021-03-05 14:54:53 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-03-05 14:54:53 | INFO | train | epoch 038 | loss 3.969 | nll_loss 2.22 | ppl 4.66 | wps 30599.4 | ups 2.17 | wpb 14123.4 | bsz 541.2 | num_updates 11700 | lr 0.000292353 | gnorm 0.709 | loss_scale 32 | train_wall 134 | wall 10540
2021-03-05 14:54:53 | INFO | fairseq.trainer | begin training epoch 39
2021-03-05 14:55:33 | INFO | train_inner | epoch 039:    100 / 308 loss=3.89, nll_loss=2.127, ppl=4.37, wps=30850.1, ups=2.18, wpb=14158.8, bsz=543, num_updates=11800, lr=0.000291111, gnorm=0.705, loss_scale=32, train_wall=38, wall=10580
2021-03-05 14:56:11 | INFO | train_inner | epoch 039:    200 / 308 loss=3.942, nll_loss=2.188, ppl=4.56, wps=36869.6, ups=2.61, wpb=14115.8, bsz=546.8, num_updates=11900, lr=0.000289886, gnorm=0.711, loss_scale=32, train_wall=38, wall=10618
2021-03-05 14:56:49 | INFO | train_inner | epoch 039:    300 / 308 loss=3.991, nll_loss=2.245, ppl=4.74, wps=36718.5, ups=2.61, wpb=14088, bsz=529.5, num_updates=12000, lr=0.000288675, gnorm=0.723, loss_scale=32, train_wall=38, wall=10656
2021-03-05 14:56:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 15:00:33 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.155 | nll_loss 3.442 | ppl 10.87 | bleu 10.82 | wps 2773.4 | wpb 12732.6 | bsz 485.9 | num_updates 12000 | best_bleu 10.86
2021-03-05 15:00:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:00:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_39_12000.pt (epoch 39 @ 12000 updates, score 10.82) (writing took 6.903716302942485 seconds)
2021-03-05 15:00:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 15:05:55 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.151 | nll_loss 3.443 | ppl 10.87 | bleu 10.73 | wps 2063.7 | wpb 12732.6 | bsz 485.9 | num_updates 12008 | best_bleu 10.86
2021-03-05 15:05:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:06:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint39.pt (epoch 39 @ 12008 updates, score 10.73) (writing took 7.25201174011454 seconds)
2021-03-05 15:06:03 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-03-05 15:06:03 | INFO | train | epoch 039 | loss 3.941 | nll_loss 2.187 | ppl 4.55 | wps 6498.1 | ups 0.46 | wpb 14123.4 | bsz 541.2 | num_updates 12008 | lr 0.000288579 | gnorm 0.713 | loss_scale 32 | train_wall 118 | wall 11210
2021-03-05 15:06:03 | INFO | fairseq.trainer | begin training epoch 40
2021-03-05 15:06:42 | INFO | train_inner | epoch 040:     92 / 308 loss=3.876, nll_loss=2.11, ppl=4.32, wps=2375.8, ups=0.17, wpb=14072.6, bsz=526.9, num_updates=12100, lr=0.00028748, gnorm=0.704, loss_scale=32, train_wall=41, wall=11249
2021-03-05 15:07:24 | INFO | train_inner | epoch 040:    192 / 308 loss=3.916, nll_loss=2.158, ppl=4.46, wps=33824.5, ups=2.39, wpb=14144.3, bsz=547.1, num_updates=12200, lr=0.000286299, gnorm=0.72, loss_scale=32, train_wall=42, wall=11291
2021-03-05 15:08:05 | INFO | train_inner | epoch 040:    292 / 308 loss=3.95, nll_loss=2.198, ppl=4.59, wps=34326.3, ups=2.42, wpb=14181.4, bsz=556.8, num_updates=12300, lr=0.000285133, gnorm=0.716, loss_scale=32, train_wall=41, wall=11332
2021-03-05 15:08:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:08:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint40.pt (epoch 40 @ 12316 updates, score None) (writing took 7.38977515976876 seconds)
2021-03-05 15:08:19 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-03-05 15:08:19 | INFO | train | epoch 040 | loss 3.916 | nll_loss 2.157 | ppl 4.46 | wps 31971.7 | ups 2.26 | wpb 14123.4 | bsz 541.2 | num_updates 12316 | lr 0.000284948 | gnorm 0.714 | loss_scale 32 | train_wall 127 | wall 11346
2021-03-05 15:08:19 | INFO | fairseq.trainer | begin training epoch 41
2021-03-05 15:08:54 | INFO | train_inner | epoch 041:     84 / 308 loss=3.857, nll_loss=2.089, ppl=4.25, wps=28732.9, ups=2.03, wpb=14187.3, bsz=536.1, num_updates=12400, lr=0.000283981, gnorm=0.705, loss_scale=32, train_wall=40, wall=11381
2021-03-05 15:09:35 | INFO | train_inner | epoch 041:    184 / 308 loss=3.881, nll_loss=2.117, ppl=4.34, wps=34766.9, ups=2.45, wpb=14207.5, bsz=549.6, num_updates=12500, lr=0.000282843, gnorm=0.709, loss_scale=32, train_wall=41, wall=11422
2021-03-05 15:10:17 | INFO | train_inner | epoch 041:    284 / 308 loss=3.933, nll_loss=2.177, ppl=4.52, wps=33501.7, ups=2.4, wpb=13985, bsz=532.1, num_updates=12600, lr=0.000281718, gnorm=0.732, loss_scale=32, train_wall=42, wall=11464
2021-03-05 15:10:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:10:34 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint41.pt (epoch 41 @ 12624 updates, score None) (writing took 6.912778133992106 seconds)
2021-03-05 15:10:34 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-03-05 15:10:34 | INFO | train | epoch 041 | loss 3.89 | nll_loss 2.127 | ppl 4.37 | wps 32050.6 | ups 2.27 | wpb 14123.4 | bsz 541.2 | num_updates 12624 | lr 0.00028145 | gnorm 0.715 | loss_scale 32 | train_wall 127 | wall 11481
2021-03-05 15:10:35 | INFO | fairseq.trainer | begin training epoch 42
2021-03-05 15:11:07 | INFO | train_inner | epoch 042:     76 / 308 loss=3.846, nll_loss=2.076, ppl=4.21, wps=28200.8, ups=2, wpb=14129.8, bsz=529.2, num_updates=12700, lr=0.000280607, gnorm=0.708, loss_scale=32, train_wall=42, wall=11514
2021-03-05 15:11:49 | INFO | train_inner | epoch 042:    176 / 308 loss=3.853, nll_loss=2.083, ppl=4.24, wps=33872.9, ups=2.4, wpb=14133.4, bsz=544.2, num_updates=12800, lr=0.000279508, gnorm=0.716, loss_scale=32, train_wall=42, wall=11556
2021-03-05 15:12:30 | INFO | train_inner | epoch 042:    276 / 308 loss=3.892, nll_loss=2.13, ppl=4.38, wps=34046.9, ups=2.41, wpb=14140.9, bsz=556.1, num_updates=12900, lr=0.000278423, gnorm=0.721, loss_scale=32, train_wall=41, wall=11597
2021-03-05 15:12:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 15:17:51 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.183 | nll_loss 3.469 | ppl 11.07 | bleu 10.93 | wps 2101.7 | wpb 12732.6 | bsz 485.9 | num_updates 12932 | best_bleu 10.93
2021-03-05 15:17:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:18:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint42.pt (epoch 42 @ 12932 updates, score 10.93) (writing took 11.918023528065532 seconds)
2021-03-05 15:18:03 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-03-05 15:18:03 | INFO | train | epoch 042 | loss 3.865 | nll_loss 2.098 | ppl 4.28 | wps 9697.5 | ups 0.69 | wpb 14123.4 | bsz 541.2 | num_updates 12932 | lr 0.000278078 | gnorm 0.715 | loss_scale 32 | train_wall 127 | wall 11930
2021-03-05 15:18:03 | INFO | fairseq.trainer | begin training epoch 43
2021-03-05 15:18:33 | INFO | train_inner | epoch 043:     68 / 308 loss=3.843, nll_loss=2.07, ppl=4.2, wps=3878, ups=0.28, wpb=14046.5, bsz=522.9, num_updates=13000, lr=0.00027735, gnorm=0.713, loss_scale=32, train_wall=41, wall=11960
2021-03-05 15:19:13 | INFO | train_inner | epoch 043:    168 / 308 loss=3.818, nll_loss=2.043, ppl=4.12, wps=34587.8, ups=2.45, wpb=14134.4, bsz=558.2, num_updates=13100, lr=0.000276289, gnorm=0.715, loss_scale=32, train_wall=41, wall=12000
2021-03-05 15:19:55 | INFO | train_inner | epoch 043:    268 / 308 loss=3.871, nll_loss=2.104, ppl=4.3, wps=33773.4, ups=2.39, wpb=14112.1, bsz=543.9, num_updates=13200, lr=0.000275241, gnorm=0.725, loss_scale=32, train_wall=42, wall=12042
2021-03-05 15:20:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:20:24 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint43.pt (epoch 43 @ 13240 updates, score None) (writing took 12.38576210476458 seconds)
2021-03-05 15:20:24 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-03-05 15:20:24 | INFO | train | epoch 043 | loss 3.842 | nll_loss 2.071 | ppl 4.2 | wps 30785.9 | ups 2.18 | wpb 14123.4 | bsz 541.2 | num_updates 13240 | lr 0.000274825 | gnorm 0.718 | loss_scale 32 | train_wall 127 | wall 12071
2021-03-05 15:20:24 | INFO | fairseq.trainer | begin training epoch 44
2021-03-05 15:20:51 | INFO | train_inner | epoch 044:     60 / 308 loss=3.829, nll_loss=2.054, ppl=4.15, wps=25332, ups=1.8, wpb=14092.9, bsz=533, num_updates=13300, lr=0.000274204, gnorm=0.72, loss_scale=32, train_wall=42, wall=12098
2021-03-05 15:21:32 | INFO | train_inner | epoch 044:    160 / 308 loss=3.8, nll_loss=2.022, ppl=4.06, wps=34577.4, ups=2.45, wpb=14133.9, bsz=569.4, num_updates=13400, lr=0.000273179, gnorm=0.719, loss_scale=64, train_wall=41, wall=12139
2021-03-05 15:22:13 | INFO | train_inner | epoch 044:    260 / 308 loss=3.836, nll_loss=2.064, ppl=4.18, wps=34775.4, ups=2.44, wpb=14229.5, bsz=539.9, num_updates=13500, lr=0.000272166, gnorm=0.719, loss_scale=64, train_wall=41, wall=12180
2021-03-05 15:22:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:22:40 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint44.pt (epoch 44 @ 13548 updates, score None) (writing took 7.659652738831937 seconds)
2021-03-05 15:22:40 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-03-05 15:22:40 | INFO | train | epoch 044 | loss 3.82 | nll_loss 2.045 | ppl 4.13 | wps 31999.5 | ups 2.27 | wpb 14123.4 | bsz 541.2 | num_updates 13548 | lr 0.000271683 | gnorm 0.725 | loss_scale 64 | train_wall 126 | wall 12207
2021-03-05 15:22:40 | INFO | fairseq.trainer | begin training epoch 45
2021-03-05 15:23:03 | INFO | train_inner | epoch 045:     52 / 308 loss=3.805, nll_loss=2.027, ppl=4.07, wps=27815.5, ups=1.99, wpb=13994.6, bsz=523.8, num_updates=13600, lr=0.000271163, gnorm=0.732, loss_scale=64, train_wall=41, wall=12230
2021-03-05 15:23:44 | INFO | train_inner | epoch 045:    152 / 308 loss=3.789, nll_loss=2.007, ppl=4.02, wps=34162.2, ups=2.41, wpb=14157, bsz=542.1, num_updates=13700, lr=0.000270172, gnorm=0.725, loss_scale=64, train_wall=41, wall=12271
2021-03-05 15:24:26 | INFO | train_inner | epoch 045:    252 / 308 loss=3.821, nll_loss=2.045, ppl=4.13, wps=33728.5, ups=2.39, wpb=14101.1, bsz=541.1, num_updates=13800, lr=0.000269191, gnorm=0.738, loss_scale=64, train_wall=42, wall=12313
2021-03-05 15:24:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 15:29:58 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.204 | nll_loss 3.49 | ppl 11.23 | bleu 10.9 | wps 2099.1 | wpb 12732.6 | bsz 485.9 | num_updates 13856 | best_bleu 10.93
2021-03-05 15:29:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:30:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint45.pt (epoch 45 @ 13856 updates, score 10.9) (writing took 7.4013970689848065 seconds)
2021-03-05 15:30:05 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-03-05 15:30:05 | INFO | train | epoch 045 | loss 3.799 | nll_loss 2.02 | ppl 4.06 | wps 9781.1 | ups 0.69 | wpb 14123.4 | bsz 541.2 | num_updates 13856 | lr 0.000268646 | gnorm 0.727 | loss_scale 64 | train_wall 127 | wall 12652
2021-03-05 15:30:05 | INFO | fairseq.trainer | begin training epoch 46
2021-03-05 15:30:25 | INFO | train_inner | epoch 046:     44 / 308 loss=3.786, nll_loss=2.005, ppl=4.01, wps=3949, ups=0.28, wpb=14150.6, bsz=528.6, num_updates=13900, lr=0.000268221, gnorm=0.72, loss_scale=64, train_wall=41, wall=12672
2021-03-05 15:31:06 | INFO | train_inner | epoch 046:    144 / 308 loss=3.762, nll_loss=1.976, ppl=3.93, wps=34333.6, ups=2.43, wpb=14152.6, bsz=540, num_updates=14000, lr=0.000267261, gnorm=0.734, loss_scale=64, train_wall=41, wall=12713
2021-03-05 15:31:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 15:36:14 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.204 | nll_loss 3.491 | ppl 11.24 | bleu 10.69 | wps 2095 | wpb 12732.6 | bsz 485.9 | num_updates 14000 | best_bleu 10.93
2021-03-05 15:36:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:36:22 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_46_14000.pt (epoch 46 @ 14000 updates, score 10.69) (writing took 7.760052936617285 seconds)
2021-03-05 15:37:03 | INFO | train_inner | epoch 046:    244 / 308 loss=3.798, nll_loss=2.018, ppl=4.05, wps=3965.5, ups=0.28, wpb=14166.7, bsz=545.2, num_updates=14100, lr=0.000266312, gnorm=0.732, loss_scale=64, train_wall=41, wall=13070
2021-03-05 15:37:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:37:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint46.pt (epoch 46 @ 14164 updates, score None) (writing took 8.313354866113514 seconds)
2021-03-05 15:37:38 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-03-05 15:37:38 | INFO | train | epoch 046 | loss 3.779 | nll_loss 1.996 | ppl 3.99 | wps 9600.6 | ups 0.68 | wpb 14123.4 | bsz 541.2 | num_updates 14164 | lr 0.000265709 | gnorm 0.734 | loss_scale 64 | train_wall 127 | wall 13105
2021-03-05 15:37:38 | INFO | fairseq.trainer | begin training epoch 47
2021-03-05 15:37:54 | INFO | train_inner | epoch 047:     36 / 308 loss=3.768, nll_loss=1.984, ppl=3.96, wps=27466.5, ups=1.96, wpb=14016.8, bsz=545.5, num_updates=14200, lr=0.000265372, gnorm=0.74, loss_scale=64, train_wall=41, wall=13121
2021-03-05 15:38:36 | INFO | train_inner | epoch 047:    136 / 308 loss=3.742, nll_loss=1.952, ppl=3.87, wps=33870.5, ups=2.41, wpb=14069.4, bsz=530.7, num_updates=14300, lr=0.000264443, gnorm=0.731, loss_scale=64, train_wall=41, wall=13163
2021-03-05 15:39:17 | INFO | train_inner | epoch 047:    236 / 308 loss=3.772, nll_loss=1.988, ppl=3.97, wps=34239.5, ups=2.42, wpb=14157.9, bsz=545.6, num_updates=14400, lr=0.000263523, gnorm=0.732, loss_scale=64, train_wall=41, wall=13204
2021-03-05 15:39:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:39:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint47.pt (epoch 47 @ 14472 updates, score None) (writing took 7.772168669383973 seconds)
2021-03-05 15:39:55 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-03-05 15:39:55 | INFO | train | epoch 047 | loss 3.759 | nll_loss 1.973 | ppl 3.92 | wps 31700.9 | ups 2.24 | wpb 14123.4 | bsz 541.2 | num_updates 14472 | lr 0.000262867 | gnorm 0.733 | loss_scale 64 | train_wall 127 | wall 13242
2021-03-05 15:39:55 | INFO | fairseq.trainer | begin training epoch 48
2021-03-05 15:40:08 | INFO | train_inner | epoch 048:     28 / 308 loss=3.761, nll_loss=1.975, ppl=3.93, wps=27453.1, ups=1.95, wpb=14067.3, bsz=538.4, num_updates=14500, lr=0.000262613, gnorm=0.737, loss_scale=64, train_wall=42, wall=13255
2021-03-05 15:40:50 | INFO | train_inner | epoch 048:    128 / 308 loss=3.713, nll_loss=1.918, ppl=3.78, wps=34212, ups=2.4, wpb=14228, bsz=534.1, num_updates=14600, lr=0.000261712, gnorm=0.723, loss_scale=64, train_wall=41, wall=13297
2021-03-05 15:41:31 | INFO | train_inner | epoch 048:    228 / 308 loss=3.747, nll_loss=1.958, ppl=3.89, wps=34535.2, ups=2.45, wpb=14107.3, bsz=548.4, num_updates=14700, lr=0.00026082, gnorm=0.734, loss_scale=64, train_wall=41, wall=13338
2021-03-05 15:42:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 15:47:13 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.222 | nll_loss 3.51 | ppl 11.4 | bleu 10.57 | wps 2079.8 | wpb 12732.6 | bsz 485.9 | num_updates 14780 | best_bleu 10.93
2021-03-05 15:47:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:47:21 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint48.pt (epoch 48 @ 14780 updates, score 10.57) (writing took 7.669547581113875 seconds)
2021-03-05 15:47:21 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-03-05 15:47:21 | INFO | train | epoch 048 | loss 3.738 | nll_loss 1.948 | ppl 3.86 | wps 9758.2 | ups 0.69 | wpb 14123.4 | bsz 541.2 | num_updates 14780 | lr 0.000260113 | gnorm 0.732 | loss_scale 64 | train_wall 126 | wall 13688
2021-03-05 15:47:21 | INFO | fairseq.trainer | begin training epoch 49
2021-03-05 15:47:31 | INFO | train_inner | epoch 049:     20 / 308 loss=3.768, nll_loss=1.983, ppl=3.95, wps=3922.9, ups=0.28, wpb=14148.3, bsz=548, num_updates=14800, lr=0.000259938, gnorm=0.739, loss_scale=64, train_wall=41, wall=13698
2021-03-05 15:48:14 | INFO | train_inner | epoch 049:    120 / 308 loss=3.679, nll_loss=1.879, ppl=3.68, wps=33432.8, ups=2.36, wpb=14139.4, bsz=550.3, num_updates=14900, lr=0.000259064, gnorm=0.73, loss_scale=64, train_wall=42, wall=13741
2021-03-05 15:48:55 | INFO | train_inner | epoch 049:    220 / 308 loss=3.736, nll_loss=1.945, ppl=3.85, wps=34484.3, ups=2.43, wpb=14173.4, bsz=543.8, num_updates=15000, lr=0.000258199, gnorm=0.75, loss_scale=64, train_wall=41, wall=13782
2021-03-05 15:49:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:49:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint49.pt (epoch 49 @ 15088 updates, score None) (writing took 7.702773320954293 seconds)
2021-03-05 15:49:39 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-03-05 15:49:39 | INFO | train | epoch 049 | loss 3.721 | nll_loss 1.928 | ppl 3.8 | wps 31611.2 | ups 2.24 | wpb 14123.4 | bsz 541.2 | num_updates 15088 | lr 0.000257445 | gnorm 0.744 | loss_scale 64 | train_wall 128 | wall 13826
2021-03-05 15:49:39 | INFO | fairseq.trainer | begin training epoch 50
2021-03-05 15:49:45 | INFO | train_inner | epoch 050:     12 / 308 loss=3.742, nll_loss=1.953, ppl=3.87, wps=27886.3, ups=1.98, wpb=14085.9, bsz=526.7, num_updates=15100, lr=0.000257343, gnorm=0.751, loss_scale=64, train_wall=41, wall=13832
2021-03-05 15:50:26 | INFO | train_inner | epoch 050:    112 / 308 loss=3.67, nll_loss=1.867, ppl=3.65, wps=34664.5, ups=2.45, wpb=14170.2, bsz=544, num_updates=15200, lr=0.000256495, gnorm=0.74, loss_scale=64, train_wall=41, wall=13873
2021-03-05 15:51:07 | INFO | train_inner | epoch 050:    212 / 308 loss=3.699, nll_loss=1.902, ppl=3.74, wps=34521.7, ups=2.44, wpb=14124.5, bsz=547.5, num_updates=15300, lr=0.000255655, gnorm=0.733, loss_scale=64, train_wall=41, wall=13914
2021-03-05 15:51:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:51:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint50.pt (epoch 50 @ 15396 updates, score None) (writing took 7.827181956730783 seconds)
2021-03-05 15:51:55 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-03-05 15:51:55 | INFO | train | epoch 050 | loss 3.701 | nll_loss 1.904 | ppl 3.74 | wps 32026.1 | ups 2.27 | wpb 14123.4 | bsz 541.2 | num_updates 15396 | lr 0.000254857 | gnorm 0.741 | loss_scale 64 | train_wall 126 | wall 13962
2021-03-05 15:51:55 | INFO | fairseq.trainer | begin training epoch 51
2021-03-05 15:51:58 | INFO | train_inner | epoch 051:      4 / 308 loss=3.74, nll_loss=1.95, ppl=3.86, wps=27851.1, ups=1.98, wpb=14075.1, bsz=537.5, num_updates=15400, lr=0.000254824, gnorm=0.752, loss_scale=64, train_wall=41, wall=13965
2021-03-05 15:52:40 | INFO | train_inner | epoch 051:    104 / 308 loss=3.643, nll_loss=1.836, ppl=3.57, wps=33836, ups=2.39, wpb=14165.9, bsz=551.4, num_updates=15500, lr=0.000254, gnorm=0.737, loss_scale=64, train_wall=42, wall=14007
2021-03-05 15:53:16 | INFO | train_inner | epoch 051:    204 / 308 loss=3.69, nll_loss=1.89, ppl=3.71, wps=39303.6, ups=2.77, wpb=14186.7, bsz=540.9, num_updates=15600, lr=0.000253185, gnorm=0.741, loss_scale=64, train_wall=36, wall=14043
2021-03-05 15:53:37 | INFO | train_inner | epoch 051:    304 / 308 loss=3.723, nll_loss=1.93, ppl=3.81, wps=66240.5, ups=4.73, wpb=14016.9, bsz=529.2, num_updates=15700, lr=0.000252377, gnorm=0.757, loss_scale=64, train_wall=21, wall=14064
2021-03-05 15:53:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 15:54:41 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.24 | nll_loss 3.529 | ppl 11.54 | bleu 10.8 | wps 10031.4 | wpb 12732.6 | bsz 485.9 | num_updates 15704 | best_bleu 10.93
2021-03-05 15:54:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:54:58 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint51.pt (epoch 51 @ 15704 updates, score 10.8) (writing took 17.090434818994254 seconds)
2021-03-05 15:54:58 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-03-05 15:54:58 | INFO | train | epoch 051 | loss 3.684 | nll_loss 1.884 | ppl 3.69 | wps 23657.6 | ups 1.68 | wpb 14123.4 | bsz 541.2 | num_updates 15704 | lr 0.000252345 | gnorm 0.745 | loss_scale 64 | train_wall 101 | wall 14145
2021-03-05 15:54:59 | INFO | fairseq.trainer | begin training epoch 52
2021-03-05 15:55:22 | INFO | train_inner | epoch 052:     96 / 308 loss=3.623, nll_loss=1.812, ppl=3.51, wps=13458.8, ups=0.95, wpb=14097.6, bsz=536.4, num_updates=15800, lr=0.000251577, gnorm=0.744, loss_scale=64, train_wall=22, wall=14169
2021-03-05 15:55:45 | INFO | train_inner | epoch 052:    196 / 308 loss=3.681, nll_loss=1.879, ppl=3.68, wps=61083.5, ups=4.31, wpb=14183, bsz=535.8, num_updates=15900, lr=0.000250785, gnorm=0.743, loss_scale=64, train_wall=23, wall=14192
2021-03-05 15:56:09 | INFO | train_inner | epoch 052:    296 / 308 loss=3.698, nll_loss=1.902, ppl=3.74, wps=57582.8, ups=4.09, wpb=14094.6, bsz=550.8, num_updates=16000, lr=0.00025, gnorm=0.748, loss_scale=64, train_wall=24, wall=14216
2021-03-05 15:56:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 15:57:18 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.246 | nll_loss 3.534 | ppl 11.59 | bleu 10.91 | wps 9267.2 | wpb 12732.6 | bsz 485.9 | num_updates 16000 | best_bleu 10.93
2021-03-05 15:57:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:57:25 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_52_16000.pt (epoch 52 @ 16000 updates, score 10.91) (writing took 7.265932358801365 seconds)
2021-03-05 15:57:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:57:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint52.pt (epoch 52 @ 16012 updates, score None) (writing took 7.3912600870244205 seconds)
2021-03-05 15:57:38 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-03-05 15:57:38 | INFO | train | epoch 052 | loss 3.668 | nll_loss 1.865 | ppl 3.64 | wps 27324.8 | ups 1.93 | wpb 14123.4 | bsz 541.2 | num_updates 16012 | lr 0.000249906 | gnorm 0.745 | loss_scale 64 | train_wall 74 | wall 14305
2021-03-05 15:57:38 | INFO | fairseq.trainer | begin training epoch 53
2021-03-05 15:58:15 | INFO | train_inner | epoch 053:     88 / 308 loss=3.618, nll_loss=1.806, ppl=3.5, wps=11268.7, ups=0.8, wpb=14154.1, bsz=525.8, num_updates=16100, lr=0.000249222, gnorm=0.736, loss_scale=64, train_wall=41, wall=14342
2021-03-05 15:58:56 | INFO | train_inner | epoch 053:    188 / 308 loss=3.638, nll_loss=1.83, ppl=3.56, wps=33626.3, ups=2.41, wpb=13971.8, bsz=548.1, num_updates=16200, lr=0.000248452, gnorm=0.749, loss_scale=64, train_wall=41, wall=14383
2021-03-05 15:59:38 | INFO | train_inner | epoch 053:    288 / 308 loss=3.693, nll_loss=1.894, ppl=3.72, wps=34362, ups=2.42, wpb=14210.4, bsz=544.3, num_updates=16300, lr=0.000247689, gnorm=0.746, loss_scale=64, train_wall=41, wall=14425
2021-03-05 15:59:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 15:59:54 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint53.pt (epoch 53 @ 16320 updates, score None) (writing took 7.363190905656666 seconds)
2021-03-05 15:59:54 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-03-05 15:59:54 | INFO | train | epoch 053 | loss 3.649 | nll_loss 1.843 | ppl 3.59 | wps 32018.4 | ups 2.27 | wpb 14123.4 | bsz 541.2 | num_updates 16320 | lr 0.000247537 | gnorm 0.743 | loss_scale 64 | train_wall 127 | wall 14440
2021-03-05 15:59:54 | INFO | fairseq.trainer | begin training epoch 54
2021-03-05 16:00:28 | INFO | train_inner | epoch 054:     80 / 308 loss=3.602, nll_loss=1.788, ppl=3.45, wps=28153.5, ups=1.99, wpb=14125.6, bsz=543.1, num_updates=16400, lr=0.000246932, gnorm=0.73, loss_scale=64, train_wall=41, wall=14475
2021-03-05 16:01:10 | INFO | train_inner | epoch 054:    180 / 308 loss=3.626, nll_loss=1.816, ppl=3.52, wps=33919.9, ups=2.4, wpb=14118.9, bsz=552.3, num_updates=16500, lr=0.000246183, gnorm=0.758, loss_scale=64, train_wall=41, wall=14517
2021-03-05 16:01:50 | INFO | train_inner | epoch 054:    280 / 308 loss=3.676, nll_loss=1.873, ppl=3.66, wps=34560.1, ups=2.45, wpb=14107.7, bsz=518.4, num_updates=16600, lr=0.00024544, gnorm=0.76, loss_scale=64, train_wall=41, wall=14557
2021-03-05 16:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 16:07:12 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.254 | nll_loss 3.544 | ppl 11.67 | bleu 10.9 | wps 2086.3 | wpb 12732.6 | bsz 485.9 | num_updates 16628 | best_bleu 10.93
2021-03-05 16:07:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:07:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint54.pt (epoch 54 @ 16628 updates, score 10.9) (writing took 7.665524058975279 seconds)
2021-03-05 16:07:20 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-03-05 16:07:20 | INFO | train | epoch 054 | loss 3.634 | nll_loss 1.825 | ppl 3.54 | wps 9753.7 | ups 0.69 | wpb 14123.4 | bsz 541.2 | num_updates 16628 | lr 0.000245234 | gnorm 0.75 | loss_scale 64 | train_wall 126 | wall 14886
2021-03-05 16:07:20 | INFO | fairseq.trainer | begin training epoch 55
2021-03-05 16:07:50 | INFO | train_inner | epoch 055:     72 / 308 loss=3.588, nll_loss=1.772, ppl=3.42, wps=3940.9, ups=0.28, wpb=14172.9, bsz=560.9, num_updates=16700, lr=0.000244704, gnorm=0.737, loss_scale=64, train_wall=40, wall=14917
2021-03-05 16:08:32 | INFO | train_inner | epoch 055:    172 / 308 loss=3.613, nll_loss=1.8, ppl=3.48, wps=33747.6, ups=2.39, wpb=14101.9, bsz=533.9, num_updates=16800, lr=0.000243975, gnorm=0.748, loss_scale=64, train_wall=42, wall=14959
2021-03-05 16:09:14 | INFO | train_inner | epoch 055:    272 / 308 loss=3.647, nll_loss=1.84, ppl=3.58, wps=34128.6, ups=2.4, wpb=14212.2, bsz=549.4, num_updates=16900, lr=0.000243252, gnorm=0.758, loss_scale=64, train_wall=41, wall=15000
2021-03-05 16:09:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:09:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint55.pt (epoch 55 @ 16936 updates, score None) (writing took 7.608431226108223 seconds)
2021-03-05 16:09:36 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-03-05 16:09:36 | INFO | train | epoch 055 | loss 3.617 | nll_loss 1.805 | ppl 3.49 | wps 31941 | ups 2.26 | wpb 14123.4 | bsz 541.2 | num_updates 16936 | lr 0.000242993 | gnorm 0.749 | loss_scale 64 | train_wall 127 | wall 15023
2021-03-05 16:09:36 | INFO | fairseq.trainer | begin training epoch 56
2021-03-05 16:10:04 | INFO | train_inner | epoch 056:     64 / 308 loss=3.586, nll_loss=1.77, ppl=3.41, wps=27905.3, ups=1.98, wpb=14063.2, bsz=557.8, num_updates=17000, lr=0.000242536, gnorm=0.743, loss_scale=64, train_wall=41, wall=15051
2021-03-05 16:10:45 | INFO | train_inner | epoch 056:    164 / 308 loss=3.599, nll_loss=1.782, ppl=3.44, wps=33799, ups=2.41, wpb=14017.8, bsz=523.7, num_updates=17100, lr=0.000241825, gnorm=0.751, loss_scale=64, train_wall=41, wall=15092
2021-03-05 16:11:27 | INFO | train_inner | epoch 056:    264 / 308 loss=3.627, nll_loss=1.816, ppl=3.52, wps=34387.9, ups=2.42, wpb=14206, bsz=540.4, num_updates=17200, lr=0.000241121, gnorm=0.76, loss_scale=64, train_wall=41, wall=15134
2021-03-05 16:11:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:11:53 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint56.pt (epoch 56 @ 17244 updates, score None) (writing took 7.6475440841168165 seconds)
2021-03-05 16:11:53 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-03-05 16:11:53 | INFO | train | epoch 056 | loss 3.602 | nll_loss 1.787 | ppl 3.45 | wps 31730.5 | ups 2.25 | wpb 14123.4 | bsz 541.2 | num_updates 17244 | lr 0.000240814 | gnorm 0.75 | loss_scale 64 | train_wall 127 | wall 15160
2021-03-05 16:11:53 | INFO | fairseq.trainer | begin training epoch 57
2021-03-05 16:12:17 | INFO | train_inner | epoch 057:     56 / 308 loss=3.576, nll_loss=1.756, ppl=3.38, wps=27879.5, ups=1.99, wpb=14041.8, bsz=527, num_updates=17300, lr=0.000240424, gnorm=0.75, loss_scale=64, train_wall=41, wall=15184
2021-03-05 16:12:58 | INFO | train_inner | epoch 057:    156 / 308 loss=3.58, nll_loss=1.761, ppl=3.39, wps=34847.8, ups=2.46, wpb=14169.9, bsz=530.4, num_updates=17400, lr=0.000239732, gnorm=0.752, loss_scale=64, train_wall=40, wall=15225
2021-03-05 16:13:39 | INFO | train_inner | epoch 057:    256 / 308 loss=3.61, nll_loss=1.797, ppl=3.48, wps=34093.4, ups=2.41, wpb=14155.9, bsz=565.6, num_updates=17500, lr=0.000239046, gnorm=0.762, loss_scale=128, train_wall=41, wall=15266
2021-03-05 16:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 16:19:11 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.294 | nll_loss 3.581 | ppl 11.97 | bleu 10.71 | wps 2091 | wpb 12732.6 | bsz 485.9 | num_updates 17552 | best_bleu 10.93
2021-03-05 16:19:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:19:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint57.pt (epoch 57 @ 17552 updates, score 10.71) (writing took 7.674155591987073 seconds)
2021-03-05 16:19:18 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-03-05 16:19:18 | INFO | train | epoch 057 | loss 3.587 | nll_loss 1.77 | ppl 3.41 | wps 9764.9 | ups 0.69 | wpb 14123.4 | bsz 541.2 | num_updates 17552 | lr 0.000238691 | gnorm 0.755 | loss_scale 128 | train_wall 127 | wall 15605
2021-03-05 16:19:18 | INFO | fairseq.trainer | begin training epoch 58
2021-03-05 16:19:40 | INFO | train_inner | epoch 058:     48 / 308 loss=3.566, nll_loss=1.745, ppl=3.35, wps=3932.8, ups=0.28, wpb=14169.8, bsz=551.2, num_updates=17600, lr=0.000238366, gnorm=0.746, loss_scale=128, train_wall=42, wall=15627
2021-03-05 16:20:21 | INFO | train_inner | epoch 058:    148 / 308 loss=3.551, nll_loss=1.726, ppl=3.31, wps=33922.2, ups=2.4, wpb=14129.6, bsz=539.8, num_updates=17700, lr=0.000237691, gnorm=0.748, loss_scale=128, train_wall=41, wall=15668
2021-03-05 16:21:03 | INFO | train_inner | epoch 058:    248 / 308 loss=3.601, nll_loss=1.785, ppl=3.45, wps=34205.6, ups=2.43, wpb=14087.7, bsz=526.1, num_updates=17800, lr=0.000237023, gnorm=0.761, loss_scale=128, train_wall=41, wall=15709
2021-03-05 16:21:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:21:34 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint58.pt (epoch 58 @ 17860 updates, score None) (writing took 7.315057339146733 seconds)
2021-03-05 16:21:34 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-03-05 16:21:34 | INFO | train | epoch 058 | loss 3.573 | nll_loss 1.753 | ppl 3.37 | wps 31986.8 | ups 2.26 | wpb 14123.4 | bsz 541.2 | num_updates 17860 | lr 0.000236624 | gnorm 0.754 | loss_scale 128 | train_wall 127 | wall 15741
2021-03-05 16:21:34 | INFO | fairseq.trainer | begin training epoch 59
2021-03-05 16:21:53 | INFO | train_inner | epoch 059:     40 / 308 loss=3.562, nll_loss=1.742, ppl=3.34, wps=27901.2, ups=1.98, wpb=14094, bsz=558.2, num_updates=17900, lr=0.00023636, gnorm=0.761, loss_scale=128, train_wall=42, wall=15760
2021-03-05 16:22:35 | INFO | train_inner | epoch 059:    140 / 308 loss=3.539, nll_loss=1.713, ppl=3.28, wps=33787.9, ups=2.39, wpb=14136.2, bsz=524.1, num_updates=18000, lr=0.000235702, gnorm=0.751, loss_scale=128, train_wall=42, wall=15802
2021-03-05 16:22:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 16:27:47 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 5.313 | nll_loss 3.602 | ppl 12.14 | bleu 10.74 | wps 2071.6 | wpb 12732.6 | bsz 485.9 | num_updates 18000 | best_bleu 10.93
2021-03-05 16:27:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:27:54 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_59_18000.pt (epoch 59 @ 18000 updates, score 10.74) (writing took 7.388099601026624 seconds)
2021-03-05 16:28:35 | INFO | train_inner | epoch 059:    240 / 308 loss=3.579, nll_loss=1.76, ppl=3.39, wps=3931.4, ups=0.28, wpb=14174.1, bsz=549, num_updates=18100, lr=0.00023505, gnorm=0.755, loss_scale=128, train_wall=41, wall=16162
2021-03-05 16:29:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:29:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint59.pt (epoch 59 @ 18168 updates, score None) (writing took 7.321965127252042 seconds)
2021-03-05 16:29:11 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-03-05 16:29:11 | INFO | train | epoch 059 | loss 3.558 | nll_loss 1.735 | ppl 3.33 | wps 9520.5 | ups 0.67 | wpb 14123.4 | bsz 541.2 | num_updates 18168 | lr 0.00023461 | gnorm 0.754 | loss_scale 128 | train_wall 128 | wall 16198
2021-03-05 16:29:11 | INFO | fairseq.trainer | begin training epoch 60
2021-03-05 16:29:26 | INFO | train_inner | epoch 060:     32 / 308 loss=3.552, nll_loss=1.73, ppl=3.32, wps=27537.1, ups=1.97, wpb=13987.3, bsz=553, num_updates=18200, lr=0.000234404, gnorm=0.753, loss_scale=128, train_wall=42, wall=16213
2021-03-05 16:30:06 | INFO | train_inner | epoch 060:    132 / 308 loss=3.53, nll_loss=1.701, ppl=3.25, wps=35183.8, ups=2.49, wpb=14132.3, bsz=536.7, num_updates=18300, lr=0.000233762, gnorm=0.758, loss_scale=128, train_wall=40, wall=16253
2021-03-05 16:30:48 | INFO | train_inner | epoch 060:    232 / 308 loss=3.551, nll_loss=1.727, ppl=3.31, wps=34075.1, ups=2.41, wpb=14166.6, bsz=545.4, num_updates=18400, lr=0.000233126, gnorm=0.753, loss_scale=128, train_wall=41, wall=16295
2021-03-05 16:31:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 16:36:36 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 5.291 | nll_loss 3.584 | ppl 11.99 | bleu 10.58 | wps 2041 | wpb 12732.6 | bsz 485.9 | num_updates 18476 | best_bleu 10.93
2021-03-05 16:36:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:36:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint60.pt (epoch 60 @ 18476 updates, score 10.58) (writing took 8.117647095117718 seconds)
2021-03-05 16:36:44 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-03-05 16:36:44 | INFO | train | epoch 060 | loss 3.545 | nll_loss 1.72 | ppl 3.29 | wps 9612 | ups 0.68 | wpb 14123.4 | bsz 541.2 | num_updates 18476 | lr 0.000232646 | gnorm 0.756 | loss_scale 128 | train_wall 126 | wall 16651
2021-03-05 16:36:44 | INFO | fairseq.trainer | begin training epoch 61
2021-03-05 16:36:55 | INFO | train_inner | epoch 061:     24 / 308 loss=3.562, nll_loss=1.741, ppl=3.34, wps=3859, ups=0.27, wpb=14170.2, bsz=537, num_updates=18500, lr=0.000232495, gnorm=0.759, loss_scale=128, train_wall=41, wall=16662
2021-03-05 16:37:37 | INFO | train_inner | epoch 061:    124 / 308 loss=3.51, nll_loss=1.678, ppl=3.2, wps=34259, ups=2.42, wpb=14179, bsz=544.8, num_updates=18600, lr=0.000231869, gnorm=0.745, loss_scale=128, train_wall=41, wall=16703
2021-03-05 16:38:18 | INFO | train_inner | epoch 061:    224 / 308 loss=3.546, nll_loss=1.72, ppl=3.29, wps=33863.1, ups=2.39, wpb=14155.8, bsz=518.5, num_updates=18700, lr=0.000231249, gnorm=0.765, loss_scale=128, train_wall=42, wall=16745
2021-03-05 16:38:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:39:00 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint61.pt (epoch 61 @ 18784 updates, score None) (writing took 7.313174813985825 seconds)
2021-03-05 16:39:00 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-03-05 16:39:00 | INFO | train | epoch 061 | loss 3.53 | nll_loss 1.702 | ppl 3.25 | wps 31906.5 | ups 2.26 | wpb 14123.4 | bsz 541.2 | num_updates 18784 | lr 0.000230731 | gnorm 0.757 | loss_scale 128 | train_wall 127 | wall 16787
2021-03-05 16:39:00 | INFO | fairseq.trainer | begin training epoch 62
2021-03-05 16:39:08 | INFO | train_inner | epoch 062:     16 / 308 loss=3.535, nll_loss=1.71, ppl=3.27, wps=28107.1, ups=2.01, wpb=13972.5, bsz=562.5, num_updates=18800, lr=0.000230633, gnorm=0.764, loss_scale=128, train_wall=41, wall=16795
2021-03-05 16:39:50 | INFO | train_inner | epoch 062:    116 / 308 loss=3.474, nll_loss=1.637, ppl=3.11, wps=34166.9, ups=2.41, wpb=14175, bsz=559.2, num_updates=18900, lr=0.000230022, gnorm=0.737, loss_scale=128, train_wall=41, wall=16837
2021-03-05 16:40:32 | INFO | train_inner | epoch 062:    216 / 308 loss=3.53, nll_loss=1.702, ppl=3.25, wps=33548.8, ups=2.37, wpb=14174, bsz=528.3, num_updates=19000, lr=0.000229416, gnorm=0.757, loss_scale=128, train_wall=42, wall=16879
2021-03-05 16:41:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:41:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint62.pt (epoch 62 @ 19092 updates, score None) (writing took 8.536392103414983 seconds)
2021-03-05 16:41:18 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-03-05 16:41:18 | INFO | train | epoch 062 | loss 3.518 | nll_loss 1.688 | ppl 3.22 | wps 31543.7 | ups 2.23 | wpb 14123.4 | bsz 541.2 | num_updates 19092 | lr 0.000228862 | gnorm 0.755 | loss_scale 128 | train_wall 127 | wall 16925
2021-03-05 16:41:18 | INFO | fairseq.trainer | begin training epoch 63
2021-03-05 16:41:23 | INFO | train_inner | epoch 063:      8 / 308 loss=3.558, nll_loss=1.735, ppl=3.33, wps=27684.2, ups=1.97, wpb=14039.7, bsz=519.2, num_updates=19100, lr=0.000228814, gnorm=0.771, loss_scale=128, train_wall=41, wall=16929
2021-03-05 16:42:04 | INFO | train_inner | epoch 063:    108 / 308 loss=3.469, nll_loss=1.63, ppl=3.1, wps=34348.5, ups=2.42, wpb=14168.3, bsz=535.6, num_updates=19200, lr=0.000228218, gnorm=0.747, loss_scale=128, train_wall=41, wall=16971
2021-03-05 16:42:46 | INFO | train_inner | epoch 063:    208 / 308 loss=3.505, nll_loss=1.673, ppl=3.19, wps=33798.1, ups=2.4, wpb=14092.1, bsz=542.6, num_updates=19300, lr=0.000227626, gnorm=0.762, loss_scale=128, train_wall=41, wall=17012
2021-03-05 16:43:26 | INFO | train_inner | epoch 063:    308 / 308 loss=3.544, nll_loss=1.719, ppl=3.29, wps=34666.8, ups=2.45, wpb=14145.1, bsz=548.6, num_updates=19400, lr=0.000227038, gnorm=0.767, loss_scale=128, train_wall=41, wall=17053
2021-03-05 16:43:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 16:48:39 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 5.311 | nll_loss 3.606 | ppl 12.17 | bleu 10.66 | wps 2066 | wpb 12732.6 | bsz 485.9 | num_updates 19400 | best_bleu 10.93
2021-03-05 16:48:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:48:47 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint63.pt (epoch 63 @ 19400 updates, score 10.66) (writing took 7.801206471864134 seconds)
2021-03-05 16:48:47 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-03-05 16:48:47 | INFO | train | epoch 063 | loss 3.504 | nll_loss 1.672 | ppl 3.19 | wps 9691.1 | ups 0.69 | wpb 14123.4 | bsz 541.2 | num_updates 19400 | lr 0.000227038 | gnorm 0.758 | loss_scale 128 | train_wall 126 | wall 17374
2021-03-05 16:48:47 | INFO | fairseq.trainer | begin training epoch 64
2021-03-05 16:49:29 | INFO | train_inner | epoch 064:    100 / 308 loss=3.445, nll_loss=1.602, ppl=3.04, wps=3885.7, ups=0.28, wpb=14108.8, bsz=541.6, num_updates=19500, lr=0.000226455, gnorm=0.748, loss_scale=128, train_wall=41, wall=17416
2021-03-05 16:50:12 | INFO | train_inner | epoch 064:    200 / 308 loss=3.494, nll_loss=1.66, ppl=3.16, wps=33328.8, ups=2.36, wpb=14107.6, bsz=548.8, num_updates=19600, lr=0.000225877, gnorm=0.762, loss_scale=128, train_wall=42, wall=17459
2021-03-05 16:50:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2021-03-05 16:50:54 | INFO | train_inner | epoch 064:    301 / 308 loss=3.533, nll_loss=1.706, ppl=3.26, wps=33754.2, ups=2.39, wpb=14128.6, bsz=533, num_updates=19700, lr=0.000225303, gnorm=0.784, loss_scale=64, train_wall=42, wall=17501
2021-03-05 16:50:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:51:10 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint64.pt (epoch 64 @ 19707 updates, score None) (writing took 13.588447427842766 seconds)
2021-03-05 16:51:10 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-03-05 16:51:10 | INFO | train | epoch 064 | loss 3.492 | nll_loss 1.658 | ppl 3.16 | wps 30292.4 | ups 2.14 | wpb 14126.1 | bsz 542 | num_updates 19707 | lr 0.000225263 | gnorm 0.764 | loss_scale 64 | train_wall 128 | wall 17517
2021-03-05 16:51:10 | INFO | fairseq.trainer | begin training epoch 65
2021-03-05 16:51:49 | INFO | train_inner | epoch 065:     93 / 308 loss=3.444, nll_loss=1.601, ppl=3.03, wps=25591.2, ups=1.8, wpb=14194.8, bsz=535.5, num_updates=19800, lr=0.000224733, gnorm=0.748, loss_scale=64, train_wall=40, wall=17556
2021-03-05 16:52:31 | INFO | train_inner | epoch 065:    193 / 308 loss=3.481, nll_loss=1.644, ppl=3.13, wps=33356.8, ups=2.36, wpb=14114.1, bsz=559.8, num_updates=19900, lr=0.000224168, gnorm=0.765, loss_scale=64, train_wall=42, wall=17598
2021-03-05 16:53:13 | INFO | train_inner | epoch 065:    293 / 308 loss=3.517, nll_loss=1.687, ppl=3.22, wps=34127, ups=2.42, wpb=14117.3, bsz=537.3, num_updates=20000, lr=0.000223607, gnorm=0.775, loss_scale=64, train_wall=41, wall=17640
2021-03-05 16:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 16:54:26 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 5.336 | nll_loss 3.632 | ppl 12.4 | bleu 10.87 | wps 8725.3 | wpb 12732.6 | bsz 485.9 | num_updates 20000 | best_bleu 10.93
2021-03-05 16:54:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:54:33 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_65_20000.pt (epoch 65 @ 20000 updates, score 10.87) (writing took 7.1170250391587615 seconds)
2021-03-05 16:54:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:54:43 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint65.pt (epoch 65 @ 20015 updates, score None) (writing took 7.027748792432249 seconds)
2021-03-05 16:54:43 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-03-05 16:54:43 | INFO | train | epoch 065 | loss 3.481 | nll_loss 1.645 | ppl 3.13 | wps 20389.4 | ups 1.44 | wpb 14123.4 | bsz 541.2 | num_updates 20015 | lr 0.000223523 | gnorm 0.763 | loss_scale 64 | train_wall 124 | wall 17730
2021-03-05 16:54:44 | INFO | fairseq.trainer | begin training epoch 66
2021-03-05 16:55:05 | INFO | train_inner | epoch 066:     85 / 308 loss=3.439, nll_loss=1.595, ppl=3.02, wps=12673.2, ups=0.9, wpb=14157.2, bsz=561.5, num_updates=20100, lr=0.00022305, gnorm=0.755, loss_scale=64, train_wall=23, wall=17751
2021-03-05 16:55:27 | INFO | train_inner | epoch 066:    185 / 308 loss=3.464, nll_loss=1.623, ppl=3.08, wps=63786.1, ups=4.54, wpb=14059.1, bsz=508.1, num_updates=20200, lr=0.000222497, gnorm=0.767, loss_scale=64, train_wall=22, wall=17774
2021-03-05 16:55:51 | INFO | train_inner | epoch 066:    285 / 308 loss=3.495, nll_loss=1.662, ppl=3.17, wps=58243.9, ups=4.11, wpb=14167.7, bsz=551.4, num_updates=20300, lr=0.000221948, gnorm=0.768, loss_scale=64, train_wall=24, wall=17798
2021-03-05 16:55:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 16:57:02 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 5.347 | nll_loss 3.644 | ppl 12.5 | bleu 10.73 | wps 9786.8 | wpb 12732.6 | bsz 485.9 | num_updates 20323 | best_bleu 10.93
2021-03-05 16:57:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:57:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint66.pt (epoch 66 @ 20323 updates, score 10.73) (writing took 10.427029644139111 seconds)
2021-03-05 16:57:13 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-03-05 16:57:13 | INFO | train | epoch 066 | loss 3.468 | nll_loss 1.63 | ppl 3.09 | wps 29119.5 | ups 2.06 | wpb 14123.4 | bsz 541.2 | num_updates 20323 | lr 0.000221823 | gnorm 0.765 | loss_scale 64 | train_wall 72 | wall 17880
2021-03-05 16:57:13 | INFO | fairseq.trainer | begin training epoch 67
2021-03-05 16:57:45 | INFO | train_inner | epoch 067:     77 / 308 loss=3.438, nll_loss=1.593, ppl=3.02, wps=12388.2, ups=0.88, wpb=14074.9, bsz=532.2, num_updates=20400, lr=0.000221404, gnorm=0.766, loss_scale=64, train_wall=37, wall=17911
2021-03-05 16:58:25 | INFO | train_inner | epoch 067:    177 / 308 loss=3.461, nll_loss=1.62, ppl=3.07, wps=34680.7, ups=2.45, wpb=14160.7, bsz=532.4, num_updates=20500, lr=0.000220863, gnorm=0.765, loss_scale=64, train_wall=41, wall=17952
2021-03-05 16:59:08 | INFO | train_inner | epoch 067:    277 / 308 loss=3.482, nll_loss=1.647, ppl=3.13, wps=33302.3, ups=2.36, wpb=14130.3, bsz=557.2, num_updates=20600, lr=0.000220326, gnorm=0.775, loss_scale=64, train_wall=42, wall=17995
2021-03-05 16:59:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 16:59:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint67.pt (epoch 67 @ 20631 updates, score None) (writing took 7.545154629275203 seconds)
2021-03-05 16:59:28 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-03-05 16:59:28 | INFO | train | epoch 067 | loss 3.457 | nll_loss 1.616 | ppl 3.07 | wps 32291.1 | ups 2.29 | wpb 14123.4 | bsz 541.2 | num_updates 20631 | lr 0.000220161 | gnorm 0.766 | loss_scale 64 | train_wall 125 | wall 18015
2021-03-05 16:59:28 | INFO | fairseq.trainer | begin training epoch 68
2021-03-05 16:59:57 | INFO | train_inner | epoch 068:     69 / 308 loss=3.432, nll_loss=1.587, ppl=3, wps=28463.2, ups=2.01, wpb=14125.7, bsz=540.6, num_updates=20700, lr=0.000219793, gnorm=0.761, loss_scale=64, train_wall=40, wall=18044
2021-03-05 17:00:40 | INFO | train_inner | epoch 068:    169 / 308 loss=3.432, nll_loss=1.587, ppl=3.01, wps=33963.9, ups=2.38, wpb=14279.6, bsz=557.8, num_updates=20800, lr=0.000219265, gnorm=0.755, loss_scale=64, train_wall=42, wall=18086
2021-03-05 17:01:21 | INFO | train_inner | epoch 068:    269 / 308 loss=3.465, nll_loss=1.626, ppl=3.09, wps=33753.3, ups=2.41, wpb=14007.3, bsz=531, num_updates=20900, lr=0.000218739, gnorm=0.781, loss_scale=64, train_wall=41, wall=18128
2021-03-05 17:01:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:01:45 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint68.pt (epoch 68 @ 20939 updates, score None) (writing took 7.5902986652217805 seconds)
2021-03-05 17:01:45 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-03-05 17:01:45 | INFO | train | epoch 068 | loss 3.445 | nll_loss 1.602 | ppl 3.04 | wps 31740.6 | ups 2.25 | wpb 14123.4 | bsz 541.2 | num_updates 20939 | lr 0.000218536 | gnorm 0.767 | loss_scale 64 | train_wall 127 | wall 18152
2021-03-05 17:01:45 | INFO | fairseq.trainer | begin training epoch 69
2021-03-05 17:02:11 | INFO | train_inner | epoch 069:     61 / 308 loss=3.42, nll_loss=1.573, ppl=2.97, wps=28095.6, ups=2, wpb=14025.6, bsz=531.2, num_updates=21000, lr=0.000218218, gnorm=0.765, loss_scale=64, train_wall=41, wall=18178
2021-03-05 17:02:52 | INFO | train_inner | epoch 069:    161 / 308 loss=3.433, nll_loss=1.588, ppl=3.01, wps=34735.6, ups=2.44, wpb=14232.2, bsz=539.8, num_updates=21100, lr=0.0002177, gnorm=0.765, loss_scale=64, train_wall=41, wall=18219
2021-03-05 17:03:34 | INFO | train_inner | epoch 069:    261 / 308 loss=3.455, nll_loss=1.614, ppl=3.06, wps=33587.7, ups=2.38, wpb=14103.4, bsz=541.2, num_updates=21200, lr=0.000217186, gnorm=0.773, loss_scale=64, train_wall=42, wall=18261
2021-03-05 17:03:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 17:09:02 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 5.35 | nll_loss 3.65 | ppl 12.55 | bleu 10.83 | wps 2097.5 | wpb 12732.6 | bsz 485.9 | num_updates 21247 | best_bleu 10.93
2021-03-05 17:09:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:09:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint69.pt (epoch 69 @ 21247 updates, score 10.83) (writing took 7.536678141914308 seconds)
2021-03-05 17:09:09 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-03-05 17:09:09 | INFO | train | epoch 069 | loss 3.434 | nll_loss 1.59 | ppl 3.01 | wps 9785.7 | ups 0.69 | wpb 14123.4 | bsz 541.2 | num_updates 21247 | lr 0.000216946 | gnorm 0.768 | loss_scale 64 | train_wall 127 | wall 18596
2021-03-05 17:09:09 | INFO | fairseq.trainer | begin training epoch 70
2021-03-05 17:09:32 | INFO | train_inner | epoch 070:     53 / 308 loss=3.417, nll_loss=1.57, ppl=2.97, wps=3917.3, ups=0.28, wpb=14033.1, bsz=539, num_updates=21300, lr=0.000216676, gnorm=0.764, loss_scale=64, train_wall=41, wall=18619
2021-03-05 17:10:14 | INFO | train_inner | epoch 070:    153 / 308 loss=3.411, nll_loss=1.562, ppl=2.95, wps=33823.5, ups=2.38, wpb=14221.6, bsz=546, num_updates=21400, lr=0.000216169, gnorm=0.761, loss_scale=64, train_wall=42, wall=18661
2021-03-05 17:10:56 | INFO | train_inner | epoch 070:    253 / 308 loss=3.436, nll_loss=1.593, ppl=3.02, wps=34101, ups=2.42, wpb=14115.3, bsz=549.8, num_updates=21500, lr=0.000215666, gnorm=0.779, loss_scale=64, train_wall=41, wall=18703
2021-03-05 17:11:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:11:27 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint70.pt (epoch 70 @ 21555 updates, score None) (writing took 8.534850847907364 seconds)
2021-03-05 17:11:27 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-03-05 17:11:27 | INFO | train | epoch 070 | loss 3.424 | nll_loss 1.578 | ppl 2.99 | wps 31522 | ups 2.23 | wpb 14123.4 | bsz 541.2 | num_updates 21555 | lr 0.00021539 | gnorm 0.772 | loss_scale 64 | train_wall 128 | wall 18734
2021-03-05 17:11:27 | INFO | fairseq.trainer | begin training epoch 71
2021-03-05 17:11:47 | INFO | train_inner | epoch 071:     45 / 308 loss=3.423, nll_loss=1.576, ppl=2.98, wps=27180.8, ups=1.94, wpb=14023.2, bsz=523.1, num_updates=21600, lr=0.000215166, gnorm=0.776, loss_scale=64, train_wall=41, wall=18754
2021-03-05 17:12:28 | INFO | train_inner | epoch 071:    145 / 308 loss=3.392, nll_loss=1.54, ppl=2.91, wps=33917.5, ups=2.43, wpb=13935.1, bsz=527.3, num_updates=21700, lr=0.000214669, gnorm=0.78, loss_scale=64, train_wall=41, wall=18795
2021-03-05 17:13:11 | INFO | train_inner | epoch 071:    245 / 308 loss=3.431, nll_loss=1.586, ppl=3, wps=33923.3, ups=2.37, wpb=14316.4, bsz=547.8, num_updates=21800, lr=0.000214176, gnorm=0.769, loss_scale=64, train_wall=42, wall=18837
2021-03-05 17:13:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:13:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint71.pt (epoch 71 @ 21863 updates, score None) (writing took 7.642396631184965 seconds)
2021-03-05 17:13:44 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-03-05 17:13:44 | INFO | train | epoch 071 | loss 3.413 | nll_loss 1.564 | ppl 2.96 | wps 31689.2 | ups 2.24 | wpb 14123.4 | bsz 541.2 | num_updates 21863 | lr 0.000213868 | gnorm 0.773 | loss_scale 64 | train_wall 128 | wall 18871
2021-03-05 17:13:45 | INFO | fairseq.trainer | begin training epoch 72
2021-03-05 17:14:01 | INFO | train_inner | epoch 072:     37 / 308 loss=3.409, nll_loss=1.561, ppl=2.95, wps=27915.2, ups=1.98, wpb=14124.1, bsz=563.9, num_updates=21900, lr=0.000213687, gnorm=0.772, loss_scale=64, train_wall=41, wall=18888
2021-03-05 17:14:43 | INFO | train_inner | epoch 072:    137 / 308 loss=3.382, nll_loss=1.526, ppl=2.88, wps=33594.9, ups=2.38, wpb=14123.5, bsz=517.8, num_updates=22000, lr=0.000213201, gnorm=0.775, loss_scale=64, train_wall=42, wall=18930
2021-03-05 17:14:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 17:19:56 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 5.38 | nll_loss 3.683 | ppl 12.84 | bleu 10.69 | wps 2064.3 | wpb 12732.6 | bsz 485.9 | num_updates 22000 | best_bleu 10.93
2021-03-05 17:19:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:20:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_72_22000.pt (epoch 72 @ 22000 updates, score 10.69) (writing took 7.537728467024863 seconds)
2021-03-05 17:20:45 | INFO | train_inner | epoch 072:    237 / 308 loss=3.421, nll_loss=1.576, ppl=2.98, wps=3926.1, ups=0.28, wpb=14193.3, bsz=573.4, num_updates=22100, lr=0.000212718, gnorm=0.783, loss_scale=64, train_wall=41, wall=19292
2021-03-05 17:21:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 17:26:29 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 5.37 | nll_loss 3.667 | ppl 12.7 | bleu 10.69 | wps 2051.8 | wpb 12732.6 | bsz 485.9 | num_updates 22171 | best_bleu 10.93
2021-03-05 17:26:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:26:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint72.pt (epoch 72 @ 22171 updates, score 10.69) (writing took 7.546940246131271 seconds)
2021-03-05 17:26:36 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-03-05 17:26:36 | INFO | train | epoch 072 | loss 3.403 | nll_loss 1.553 | ppl 2.93 | wps 5636.4 | ups 0.4 | wpb 14123.4 | bsz 541.2 | num_updates 22171 | lr 0.000212377 | gnorm 0.776 | loss_scale 64 | train_wall 127 | wall 19643
2021-03-05 17:26:36 | INFO | fairseq.trainer | begin training epoch 73
2021-03-05 17:26:50 | INFO | train_inner | epoch 073:     29 / 308 loss=3.415, nll_loss=1.566, ppl=2.96, wps=3864.2, ups=0.27, wpb=14097.6, bsz=533.2, num_updates=22200, lr=0.000212238, gnorm=0.778, loss_scale=64, train_wall=41, wall=19656
2021-03-05 17:27:30 | INFO | train_inner | epoch 073:    129 / 308 loss=3.365, nll_loss=1.507, ppl=2.84, wps=34677.2, ups=2.47, wpb=14042.1, bsz=521.4, num_updates=22300, lr=0.000211762, gnorm=0.767, loss_scale=64, train_wall=40, wall=19697
2021-03-05 17:28:11 | INFO | train_inner | epoch 073:    229 / 308 loss=3.4, nll_loss=1.551, ppl=2.93, wps=34849.9, ups=2.44, wpb=14252.8, bsz=570.9, num_updates=22400, lr=0.000211289, gnorm=0.774, loss_scale=64, train_wall=41, wall=19738
2021-03-05 17:28:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:28:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint73.pt (epoch 73 @ 22479 updates, score None) (writing took 7.628007198218256 seconds)
2021-03-05 17:28:51 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-03-05 17:28:51 | INFO | train | epoch 073 | loss 3.391 | nll_loss 1.539 | ppl 2.91 | wps 32311.6 | ups 2.29 | wpb 14123.4 | bsz 541.2 | num_updates 22479 | lr 0.000210917 | gnorm 0.775 | loss_scale 64 | train_wall 125 | wall 19778
2021-03-05 17:28:51 | INFO | fairseq.trainer | begin training epoch 74
2021-03-05 17:29:01 | INFO | train_inner | epoch 074:     21 / 308 loss=3.417, nll_loss=1.569, ppl=2.97, wps=28336.4, ups=2, wpb=14163, bsz=527.8, num_updates=22500, lr=0.000210819, gnorm=0.784, loss_scale=64, train_wall=41, wall=19788
2021-03-05 17:29:41 | INFO | train_inner | epoch 074:    121 / 308 loss=3.347, nll_loss=1.487, ppl=2.8, wps=34873.4, ups=2.47, wpb=14105.1, bsz=531.6, num_updates=22600, lr=0.000210352, gnorm=0.762, loss_scale=64, train_wall=40, wall=19828
2021-03-05 17:30:24 | INFO | train_inner | epoch 074:    221 / 308 loss=3.394, nll_loss=1.542, ppl=2.91, wps=33229.1, ups=2.36, wpb=14096.5, bsz=539.9, num_updates=22700, lr=0.000209888, gnorm=0.792, loss_scale=64, train_wall=42, wall=19871
2021-03-05 17:31:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:31:08 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint74.pt (epoch 74 @ 22787 updates, score None) (writing took 7.851223699748516 seconds)
2021-03-05 17:31:08 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-03-05 17:31:08 | INFO | train | epoch 074 | loss 3.382 | nll_loss 1.529 | ppl 2.89 | wps 31759.9 | ups 2.25 | wpb 14123.4 | bsz 541.2 | num_updates 22787 | lr 0.000209487 | gnorm 0.775 | loss_scale 64 | train_wall 127 | wall 19915
2021-03-05 17:31:08 | INFO | fairseq.trainer | begin training epoch 75
2021-03-05 17:31:14 | INFO | train_inner | epoch 075:     13 / 308 loss=3.405, nll_loss=1.556, ppl=2.94, wps=27859.5, ups=1.98, wpb=14075.5, bsz=544.4, num_updates=22800, lr=0.000209427, gnorm=0.775, loss_scale=64, train_wall=41, wall=19921
2021-03-05 17:31:55 | INFO | train_inner | epoch 075:    113 / 308 loss=3.339, nll_loss=1.478, ppl=2.79, wps=34375, ups=2.44, wpb=14109.7, bsz=533.3, num_updates=22900, lr=0.000208969, gnorm=0.766, loss_scale=64, train_wall=41, wall=19962
2021-03-05 17:32:37 | INFO | train_inner | epoch 075:    213 / 308 loss=3.37, nll_loss=1.516, ppl=2.86, wps=34132, ups=2.41, wpb=14144.9, bsz=573.8, num_updates=23000, lr=0.000208514, gnorm=0.779, loss_scale=64, train_wall=41, wall=20004
2021-03-05 17:33:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 17:38:28 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 5.401 | nll_loss 3.698 | ppl 12.98 | bleu 10.68 | wps 2065.4 | wpb 12732.6 | bsz 485.9 | num_updates 23095 | best_bleu 10.93
2021-03-05 17:38:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:38:37 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint75.pt (epoch 75 @ 23095 updates, score 10.68) (writing took 8.928315388970077 seconds)
2021-03-05 17:38:37 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-03-05 17:38:37 | INFO | train | epoch 075 | loss 3.372 | nll_loss 1.517 | ppl 2.86 | wps 9693.3 | ups 0.69 | wpb 14123.4 | bsz 541.2 | num_updates 23095 | lr 0.000208085 | gnorm 0.777 | loss_scale 64 | train_wall 126 | wall 20364
2021-03-05 17:38:37 | INFO | fairseq.trainer | begin training epoch 76
2021-03-05 17:38:40 | INFO | train_inner | epoch 076:      5 / 308 loss=3.409, nll_loss=1.56, ppl=2.95, wps=3889.2, ups=0.28, wpb=14127.1, bsz=523.2, num_updates=23100, lr=0.000208063, gnorm=0.785, loss_scale=64, train_wall=40, wall=20367
2021-03-05 17:39:22 | INFO | train_inner | epoch 076:    105 / 308 loss=3.328, nll_loss=1.465, ppl=2.76, wps=34039.3, ups=2.38, wpb=14284.2, bsz=554.9, num_updates=23200, lr=0.000207614, gnorm=0.762, loss_scale=64, train_wall=42, wall=20409
2021-03-05 17:40:04 | INFO | train_inner | epoch 076:    205 / 308 loss=3.369, nll_loss=1.512, ppl=2.85, wps=33644.9, ups=2.41, wpb=13973.7, bsz=527.2, num_updates=23300, lr=0.000207168, gnorm=0.787, loss_scale=64, train_wall=41, wall=20451
2021-03-05 17:40:45 | INFO | train_inner | epoch 076:    305 / 308 loss=3.394, nll_loss=1.543, ppl=2.91, wps=33814.5, ups=2.4, wpb=14099.9, bsz=542.6, num_updates=23400, lr=0.000206725, gnorm=0.786, loss_scale=64, train_wall=42, wall=20492
2021-03-05 17:40:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:40:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint76.pt (epoch 76 @ 23403 updates, score None) (writing took 8.276984649710357 seconds)
2021-03-05 17:40:55 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-03-05 17:40:55 | INFO | train | epoch 076 | loss 3.363 | nll_loss 1.506 | ppl 2.84 | wps 31500.7 | ups 2.23 | wpb 14123.4 | bsz 541.2 | num_updates 23403 | lr 0.000206711 | gnorm 0.778 | loss_scale 64 | train_wall 128 | wall 20502
2021-03-05 17:40:55 | INFO | fairseq.trainer | begin training epoch 77
2021-03-05 17:41:36 | INFO | train_inner | epoch 077:     97 / 308 loss=3.317, nll_loss=1.453, ppl=2.74, wps=28030, ups=1.97, wpb=14197.3, bsz=537.6, num_updates=23500, lr=0.000206284, gnorm=0.764, loss_scale=64, train_wall=41, wall=20543
2021-03-05 17:42:18 | INFO | train_inner | epoch 077:    197 / 308 loss=3.361, nll_loss=1.503, ppl=2.83, wps=33952.1, ups=2.41, wpb=14107.3, bsz=528.5, num_updates=23600, lr=0.000205847, gnorm=0.79, loss_scale=64, train_wall=41, wall=20584
2021-03-05 17:43:00 | INFO | train_inner | epoch 077:    297 / 308 loss=3.381, nll_loss=1.528, ppl=2.88, wps=33360.5, ups=2.38, wpb=14016.5, bsz=551.4, num_updates=23700, lr=0.000205412, gnorm=0.79, loss_scale=64, train_wall=42, wall=20626
2021-03-05 17:43:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:43:12 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint77.pt (epoch 77 @ 23711 updates, score None) (writing took 7.691850502975285 seconds)
2021-03-05 17:43:12 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-03-05 17:43:12 | INFO | train | epoch 077 | loss 3.354 | nll_loss 1.496 | ppl 2.82 | wps 31779.6 | ups 2.25 | wpb 14123.4 | bsz 541.2 | num_updates 23711 | lr 0.000205364 | gnorm 0.781 | loss_scale 128 | train_wall 127 | wall 20639
2021-03-05 17:43:12 | INFO | fairseq.trainer | begin training epoch 78
2021-03-05 17:43:50 | INFO | train_inner | epoch 078:     89 / 308 loss=3.315, nll_loss=1.449, ppl=2.73, wps=28215.2, ups=1.99, wpb=14170.5, bsz=536.6, num_updates=23800, lr=0.00020498, gnorm=0.762, loss_scale=128, train_wall=41, wall=20677
2021-03-05 17:44:31 | INFO | train_inner | epoch 078:    189 / 308 loss=3.348, nll_loss=1.488, ppl=2.81, wps=34194.8, ups=2.41, wpb=14195.9, bsz=554.1, num_updates=23900, lr=0.000204551, gnorm=0.783, loss_scale=128, train_wall=41, wall=20718
2021-03-05 17:45:12 | INFO | train_inner | epoch 078:    289 / 308 loss=3.37, nll_loss=1.515, ppl=2.86, wps=34528.7, ups=2.44, wpb=14130.5, bsz=538.2, num_updates=24000, lr=0.000204124, gnorm=0.788, loss_scale=128, train_wall=41, wall=20759
2021-03-05 17:45:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 17:50:23 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 5.418 | nll_loss 3.717 | ppl 13.15 | bleu 10.79 | wps 2082.9 | wpb 12732.6 | bsz 485.9 | num_updates 24000 | best_bleu 10.93
2021-03-05 17:50:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:50:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_78_24000.pt (epoch 78 @ 24000 updates, score 10.79) (writing took 7.7771870526485145 seconds)
2021-03-05 17:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 17:53:13 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 5.42 | nll_loss 3.716 | ppl 13.14 | bleu 10.82 | wps 4420.1 | wpb 12732.6 | bsz 485.9 | num_updates 24019 | best_bleu 10.93
2021-03-05 17:53:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:53:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint78.pt (epoch 78 @ 24019 updates, score 10.82) (writing took 7.16801935993135 seconds)
2021-03-05 17:53:20 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-03-05 17:53:20 | INFO | train | epoch 078 | loss 3.344 | nll_loss 1.484 | ppl 2.8 | wps 7148.2 | ups 0.51 | wpb 14123.4 | bsz 541.2 | num_updates 24019 | lr 0.000204043 | gnorm 0.779 | loss_scale 128 | train_wall 126 | wall 21247
2021-03-05 17:53:20 | INFO | fairseq.trainer | begin training epoch 79
2021-03-05 17:53:39 | INFO | train_inner | epoch 079:     81 / 308 loss=3.311, nll_loss=1.445, ppl=2.72, wps=2778.8, ups=0.2, wpb=14085.3, bsz=538.9, num_updates=24100, lr=0.0002037, gnorm=0.77, loss_scale=128, train_wall=25, wall=21266
2021-03-05 17:54:00 | INFO | train_inner | epoch 079:    181 / 308 loss=3.335, nll_loss=1.472, ppl=2.77, wps=68342.5, ups=4.81, wpb=14201.3, bsz=540.8, num_updates=24200, lr=0.000203279, gnorm=0.78, loss_scale=128, train_wall=21, wall=21287
2021-03-05 17:54:21 | INFO | train_inner | epoch 079:    281 / 308 loss=3.36, nll_loss=1.503, ppl=2.83, wps=65716.2, ups=4.69, wpb=14012.9, bsz=531, num_updates=24300, lr=0.00020286, gnorm=0.797, loss_scale=128, train_wall=21, wall=21308
2021-03-05 17:54:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:54:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint79.pt (epoch 79 @ 24327 updates, score None) (writing took 10.439307319931686 seconds)
2021-03-05 17:54:38 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-03-05 17:54:38 | INFO | train | epoch 079 | loss 3.336 | nll_loss 1.474 | ppl 2.78 | wps 55969.2 | ups 3.96 | wpb 14123.4 | bsz 541.2 | num_updates 24327 | lr 0.000202748 | gnorm 0.783 | loss_scale 128 | train_wall 65 | wall 21325
2021-03-05 17:54:38 | INFO | fairseq.trainer | begin training epoch 80
2021-03-05 17:54:56 | INFO | train_inner | epoch 080:     73 / 308 loss=3.308, nll_loss=1.442, ppl=2.72, wps=40790, ups=2.88, wpb=14156.9, bsz=562.6, num_updates=24400, lr=0.000202444, gnorm=0.777, loss_scale=128, train_wall=23, wall=21343
2021-03-05 17:55:21 | INFO | train_inner | epoch 080:    173 / 308 loss=3.317, nll_loss=1.453, ppl=2.74, wps=57202.3, ups=4.05, wpb=14107, bsz=546, num_updates=24500, lr=0.000202031, gnorm=0.78, loss_scale=128, train_wall=24, wall=21368
2021-03-05 17:55:45 | INFO | train_inner | epoch 080:    273 / 308 loss=3.353, nll_loss=1.494, ppl=2.82, wps=58124.2, ups=4.14, wpb=14028.6, bsz=528.1, num_updates=24600, lr=0.000201619, gnorm=0.792, loss_scale=128, train_wall=24, wall=21392
2021-03-05 17:55:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 17:56:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint80.pt (epoch 80 @ 24635 updates, score None) (writing took 10.278563286177814 seconds)
2021-03-05 17:56:03 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-03-05 17:56:03 | INFO | train | epoch 080 | loss 3.327 | nll_loss 1.464 | ppl 2.76 | wps 51115.5 | ups 3.62 | wpb 14123.4 | bsz 541.2 | num_updates 24635 | lr 0.000201476 | gnorm 0.781 | loss_scale 128 | train_wall 73 | wall 21410
2021-03-05 17:56:03 | INFO | fairseq.trainer | begin training epoch 81
2021-03-05 17:56:19 | INFO | train_inner | epoch 081:     65 / 308 loss=3.314, nll_loss=1.448, ppl=2.73, wps=41035.2, ups=2.88, wpb=14230.3, bsz=528, num_updates=24700, lr=0.000201211, gnorm=0.772, loss_scale=128, train_wall=23, wall=21426
2021-03-05 17:56:45 | INFO | train_inner | epoch 081:    165 / 308 loss=3.308, nll_loss=1.442, ppl=2.72, wps=54744.5, ups=3.87, wpb=14160.1, bsz=560.8, num_updates=24800, lr=0.000200805, gnorm=0.775, loss_scale=128, train_wall=26, wall=21452
2021-03-05 17:57:26 | INFO | train_inner | epoch 081:    265 / 308 loss=3.337, nll_loss=1.476, ppl=2.78, wps=34479.6, ups=2.45, wpb=14099.1, bsz=540.6, num_updates=24900, lr=0.000200401, gnorm=0.787, loss_scale=128, train_wall=41, wall=21493
2021-03-05 17:57:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 18:02:57 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 5.419 | nll_loss 3.722 | ppl 13.2 | bleu 10.67 | wps 2067.2 | wpb 12732.6 | bsz 485.9 | num_updates 24943 | best_bleu 10.93
2021-03-05 18:02:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 18:03:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint81.pt (epoch 81 @ 24943 updates, score 10.67) (writing took 8.018068057019264 seconds)
2021-03-05 18:03:05 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-03-05 18:03:05 | INFO | train | epoch 081 | loss 3.319 | nll_loss 1.455 | ppl 2.74 | wps 10313.4 | ups 0.73 | wpb 14123.4 | bsz 541.2 | num_updates 24943 | lr 0.000200228 | gnorm 0.782 | loss_scale 128 | train_wall 99 | wall 21832
2021-03-05 18:03:05 | INFO | fairseq.trainer | begin training epoch 82
2021-03-05 18:03:30 | INFO | train_inner | epoch 082:     57 / 308 loss=3.302, nll_loss=1.435, ppl=2.7, wps=3883.6, ups=0.28, wpb=14113.3, bsz=532.6, num_updates=25000, lr=0.0002, gnorm=0.783, loss_scale=128, train_wall=41, wall=21857
2021-03-05 18:03:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-05 18:08:38 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 5.447 | nll_loss 3.749 | ppl 13.45 | bleu 10.76 | wps 2092.5 | wpb 12732.6 | bsz 485.9 | num_updates 25000 | best_bleu 10.93
2021-03-05 18:08:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-03-05 18:08:54 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer_wmt_en_zh_big/checkpoint_last.pt (epoch 82 @ 25000 updates, score 10.76) (writing took 15.561695732176304 seconds)
2021-03-05 18:08:54 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-03-05 18:08:54 | INFO | train | epoch 082 | loss 3.26 | nll_loss 1.387 | ppl 2.61 | wps 2318.3 | ups 0.16 | wpb 14201.8 | bsz 540.2 | num_updates 25000 | lr 0.0002 | gnorm 0.763 | loss_scale 128 | train_wall 23 | wall 22181
2021-03-05 18:08:54 | INFO | fairseq_cli.train | done training in 22180.6 seconds
evaluation on average_checkpoints 10:
Generate test with beam=5: BLEU4 = 22.02, 57.3/28.4/16.2/9.7 (BP=0.979, ratio=0.979, syslen=1178764, reflen=1203783)
evaluation on best checkpoint:
Generate test with beam=5: BLEU4 = 21.80, 57.8/28.7/16.3/9.7 (BP=0.963, ratio=0.964, syslen=1160415, reflen=1203783)
